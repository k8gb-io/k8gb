{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"K8GB - Kubernetes Global Balancer <p>CNCF Sandbox Project | Roadmap | Join #k8gb on CNCF Slack</p> <p> </p> <p>A Global Service Load Balancing solution with a focus on having cloud native qualities and work natively in a Kubernetes context.</p> <p>Just a single Gslb CRD to enable the Global Load Balancing:</p> <pre><code>apiVersion: k8gb.absa.oss/v1beta1\nkind: Gslb\nmetadata:\n  name: test-gslb-failover\n  namespace: test-gslb\nspec:\n  resourceRef:\n    apiVersion: networking.k8s.io/v1\n    kind: Ingress\n    matchLabels: # ingresses.networking.k8s.io resource selector\n      app: test-gslb-failover\n  strategy:\n    type: failover # Global load balancing strategy\n    primaryGeoTag: eu-west-1 # Primary cluster geo tag\n</code></pre> <p>Global load balancing, commonly referred to as GSLB (Global Server Load Balancing) solutions, has been typically the domain of proprietary network software and hardware vendors and installed and managed by siloed network teams.</p> <p>k8gb is a completely open source, cloud native, global load balancing solution for Kubernetes.</p> <p>k8gb focuses on load balancing traffic across geographically dispersed Kubernetes clusters using multiple load balancing strategies to meet requirements such as region failover for high availability.</p> <p>Global load balancing for any Kubernetes Service can now be enabled and managed by any operations or development teams in the same Kubernetes native way as any other custom resource.</p>"},{"location":"#key-differentiators","title":"Key Differentiators","text":"<ul> <li>Load balancing is based on timeproof DNS protocol which is perfect for global scope and extremely reliable</li> <li>No dedicated management cluster and no single point of failure</li> <li>Kubernetes native application health checks utilizing status of Liveness and Readiness probes for load balancing decisions</li> <li>Configuration with a single Kubernetes CRD of Gslb kind</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Simply run</p> <pre><code>make deploy-full-local-setup\n</code></pre> <p>It will deploy two local k3s clusters via k3d, expose associated CoreDNS service for UDP DNS traffic, and install k8gb with test applications and two sample Gslb resources on top.</p> <p>This setup is adapted for local scenarios and works without external DNS provider dependency.</p> <p>Consult with local playground documentation to learn all the details of experimenting with local setup.</p> <p>Optionally, you can run <code>make deploy-prometheus</code> and check the metrics on the test clusters (http://localhost:9080, http://localhost:9081).</p>"},{"location":"#motivation-and-architecture","title":"Motivation and Architecture","text":"<p>k8gb was born out of the need for an open source, cloud native GSLB solution at Absa Group in South Africa.</p> <p>As part of the bank's wider container adoption running multiple, geographically dispersed Kubernetes clusters, the need for a global load balancer that was driven from the health of Kubernetes Services was required and for which there did not seem to be an existing solution.</p> <p>Yes, there are proprietary network software and hardware vendors with GSLB solutions and products, however, these were costly, heavyweight in terms of complexity and adoption, and were not Kubernetes native in most cases, requiring dedicated hardware or software to be run outside of Kubernetes.</p> <p>This was the problem we set out to solve with k8gb.</p> <p>Born as a completely open source project and following the popular Kubernetes operator pattern, k8gb can be installed in a Kubernetes cluster and via a Gslb custom resource, can provide independent GSLB capability to any Ingress or Service in the cluster, without the need for handoffs and coordination between dedicated network teams.</p> <p>k8gb commoditizes GSLB for Kubernetes, putting teams in complete control of exposing Services across geographically dispersed Kubernetes clusters across public and private clouds.</p> <p>k8gb requires no specialized software or hardware, relying completely on other OSS/CNCF projects, has no single point of failure, and fits in with any existing Kubernetes deployment workflow (e.g. GitOps, Kustomize, Helm, etc.) or tools.</p> <p>Please see the extended architecture documentation here</p> <p>Internal k8gb architecture and its components are described here</p>"},{"location":"#installation-and-configuration-tutorials","title":"Installation and Configuration Tutorials","text":"<p>For comprehensive installation and configuration guides, see the Installation and Configuration Tutorials documentation.</p>"},{"location":"#adopters","title":"Adopters","text":"<p>A list of publicly known users of the K8GB project can be found in ADOPTERS.md. We encourage all users of K8GB to add themselves to this list!</p>"},{"location":"#production-readiness","title":"Production Readiness","text":"<p>You can use k8gb in on-prem, cloud and hybrid environments.</p> <p>k8gb is tested with the following environment options.</p> Type Implementation Kubernetes Version &gt;= <code>1.21</code> Environment Any conformant Kubernetes cluster on-prem or in cloud Ingress Controller NGINX, AWS Load Balancer Controller * EdgeDNS Infoblox, Route53, NS1, CloudFlare, AzureDNS <p>  * We only mention solutions where we have tested and verified a k8gb installation. * If your Kubernetes version or Ingress controller is not included in the table above, it does not mean that k8gb will not work for you. k8gb is architected to run on top of any compliant Kubernetes cluster and Ingress controller.</p>"},{"location":"#presentations-featuring-k8gb","title":"Presentations Featuring k8gb","text":"KubeCon China 2025 KubeCon EU 2025 ChatLoopBackOff - Episode 42 (K8gb) KubeCon NA 2024 Open Source Summit EU 2024 KubeCon EU 2024 KubeCon NA 2023 KubeCon EU 2023 FOSDEM 2022 KCDBengaluru 2023 Crossplane Community Day KubeCon NA 2021 #29 DoK Community AWS Containers from the Couch show OpenShift Commons Briefings Demo at Kubernetes SIG Multicluster <p>You can also find recordings from our community meetings at k8gb youtube channel.</p>"},{"location":"#online-publications-featuring-k8gb","title":"Online Publications Featuring k8gb","text":"<ul> <li>https://oilbeater.com/en/2024/04/18/k8gb-best-cloudnative-gslb/</li> <li>https://www.redhat.com/en/blog/global-load-balancing-red-hat-openshift-k8gb</li> <li>https://andrewbaker.ninja/2021/01/22/external-k8gb-presentation-to-kubernetes-sig-multicluster/</li> </ul>"},{"location":"#and-even-books-featuring-k8gb","title":"And Even Books Featuring k8gb :)","text":"Kubernetes - An Enterprise Guide - Second Edition Kubernetes \u2013 An Enterprise Guide - Third Edition"},{"location":"#contributing","title":"Contributing","text":"<p>See CONTRIBUTING</p>"},{"location":"ADOPTERS/","title":"Adopters","text":"<p>This list captures the set of organizations that are using K8GB within their environments. If you are an adopter of K8GB and not yet on this list, we encourage you to add your organization here as well!</p> <p>Contributing to this list is a small effort that has a big impact to the project's growth, maturity, and momentum.  Thank you to all adopters and contributors of the K8GB project!</p>"},{"location":"ADOPTERS/#k8gb-adopters","title":"K8GB Adopters","text":"<p>This list is sorted in the order that organizations were added to it.</p> Organization Contact Description of Use Absa @kuritka K8GB was created in Absa and had there first production use in cross-regional datacenter context Millennium bcp @infbase Multicloud and multi-region cloud native load balancing Eficode @punasusi As a cloud-native and devops consulting firm, we have used k8gb in customer engagements Open Systems @abaguas Multi-cluster load balancing in private WAN PagBank @altieresfreitas Multicloud global load balancing across multiple regions Darede @diego7marques As a cloud consulting company, we support customers using k8gb"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#contributing-to-k8gb","title":"Contributing to k8gb","text":"<ul> <li>Getting started</li> <li>Getting help</li> <li>Reporting issues</li> <li>Contribution flow</li> <li>Local setup</li> <li>Deploy k8gb locally</li> <li>Upgrade k8gb to local candidate</li> <li>Overriding dev environment settings</li> <li>Testing</li> <li>Unit testing</li> <li>Testing against real k8s clusters</li> <li>Debugging</li> <li>Metrics</li> <li>Code style</li> <li>Logging</li> <li>Error handling</li> <li>Commit and Pull Request message</li> <li>Signature</li> <li>Changelog</li> <li>Documentation</li> <li>k8gb.io website</li> <li>Local website authoring and testing<ul> <li>Using custom SSL CA certificate</li> <li>.env support</li> </ul> </li> <li>End-to-end demo helper</li> <li>Release process</li> <li>Signed releases</li> <li>Software bill of materials</li> </ul> <p>k8gb is licensed under Apache 2 License and accepts contributions via GitHub pull requests. This document outlines the resources and guidelines necessary to follow by contributors to the k8gb project.</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting started","text":"<ul> <li>Fork the repository on GitHub</li> <li>See the local playground guide for local dev environment setup</li> </ul>"},{"location":"CONTRIBUTING/#getting-help","title":"Getting help","text":"<p>Feel free to ask for help and join the discussions at k8gb community discussions forum. We have dedicated <code>#k8gb</code> channel on Cloud Native Computing Foundation (CNCF) Slack, and we can also actively monitoring <code>#sig-multicluster</code> channel on Kubernetes Slack.</p>"},{"location":"CONTRIBUTING/#reporting-issues","title":"Reporting issues","text":"<p>Reporting bugs is one of the best ways to contribute. Feel free to open an issue describing your problem or question.</p>"},{"location":"CONTRIBUTING/#contribution-flow","title":"Contribution flow","text":"<p>Following is a rough outline for the contributor's workflow:</p> <ul> <li>Create a topic branch from where to base the contribution.</li> <li>Make commits of logical units.</li> <li>Make sure your code is clean and follows the code style and logging guidelines.</li> <li>Make sure the commit messages are in the proper format.</li> <li>Make sure the changes are covered by reasonable amount of testing.</li> <li>Push changes in a topic branch to a personal fork of the repository.</li> <li>Submit a pull request to k8gb-io/k8gb GitHub repository.</li> <li>Resolve review comments.</li> <li>PR must receive an \"LGTM\" approval from at least one maintainer listed in the <code>CODEOWNERS</code> file.</li> </ul>"},{"location":"CONTRIBUTING/#local-setup","title":"Local setup","text":""},{"location":"CONTRIBUTING/#deploy-k8gb-locally","title":"Deploy k8gb locally","text":"<p><pre><code>make deploy-full-local-setup\n</code></pre> deploys k8gb from scratch, including:</p> <ul> <li>2 local clusters</li> <li>Stable k8gb helm chart</li> <li>Gslb Custom Resources examples</li> <li>Test applications</li> </ul>"},{"location":"CONTRIBUTING/#upgrade-k8gb-to-local-candidate","title":"Upgrade k8gb to local candidate","text":"<p><pre><code>make upgrade-candidate\n</code></pre> performs upgrade of k8gb helm chart and controller to the testing version built from your current development tree.</p>"},{"location":"CONTRIBUTING/#overriding-dev-environment-settings","title":"Overriding dev environment settings","text":"<p>Sometimes there is a need to override environment variables used by <code>make</code> targets for local k8gb development. This can be easily achieved by providing the list of environment variables with respective values in the <code>.env</code> file at the local repository root: <pre><code>cat .env\n\n# .env:\nLOG_LEVEL=info\nLOG_FORMAT=json\n</code></pre> Overrides done this way can persist between terminal sessions and can be used as a single point of configuration for development in terminal and IDE of choice.</p>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":"<ul> <li>Any functional GSLB controller code change should be secured by the corresponding unit tests.</li> <li>Integration terratest suite is located here.   These tests are updated only if the change is substantial enough to affect the main end-to-end flow.</li> <li>See the local playground guide for local testing environment setup and integration test execution.</li> </ul>"},{"location":"CONTRIBUTING/#unit-testing","title":"Unit testing","text":"<ul> <li>Include unit tests when you contribute new features, as they help to a) prove that your code works correctly, and b) guard against future breaking changes to lower the maintenance cost.</li> <li>Bug fixes also generally require unit tests, because the presence of bugs usually indicates insufficient test coverage.</li> </ul> <p>Use <code>make test</code> to check your implementation changes.</p>"},{"location":"CONTRIBUTING/#testing-against-real-k8s-clusters","title":"Testing against real k8s clusters","text":"<p>There is a possibility to execute the integration terratest suite over the real clusters. For this, you need to override the set of test settings as in the example below. <pre><code>PRIMARY_GEO_TAG=af-south-1 \\\nSECONDARY_GEO_TAG=eu-west-1 \\\nDNS_SERVER1=a377095726f1845fb85b95c2afef8ac0-9a1a10f24e634e28.elb.af-south-1.amazonaws.com \\\nDNS_SERVER1_PORT=53 \\\nDNS_SERVER2=a873f5c83be624a0a84c05a743d598a8-443f7e0285e4a28f.elb.eu-west-1.amazonaws.com \\\nDNS_SERVER2_PORT=53 \\\nGSLB_DOMAIN=test.k8gb.io \\\nK8GB_CLUSTER1=arn:aws:eks:af-south-1:&lt;aws-account-id&gt;:cluster/k8gb-cluster-af-south-1 \\\nK8GB_CLUSTER2=arn:aws:eks:eu-west-1:&lt;aws-account-id&gt;:cluster/k8gb-cluster-eu-west-1 \\\nmake terratest\n</code></pre></p>"},{"location":"CONTRIBUTING/#debugging","title":"Debugging","text":"<ol> <li> <p>Install Delve debugger first. Follow the installation instructions for specific platforms from Delve's website.</p> </li> <li> <p>Run delve with options specific to IDE of choice. There is a dedicated make target available for Goland:</p> <p><pre><code>make debug-idea\n</code></pre> This article describes possible option examples for Goland and VS Code.</p> </li> <li> <p>Attach debugger of your IDE to port <code>2345</code>.</p> </li> </ol>"},{"location":"CONTRIBUTING/#metrics","title":"Metrics","text":"<p>More info about k8gb metrics can be found in the metrics.md document. If you need to check and query the k8gb metrics locally, you can install a Prometheus in the local clusters using the <code>make deploy-prometheus</code> command.</p> <p>The deployed Prometheus scrapes metrics from the dedicated k8gb operator endpoint and makes them accessible via Prometheus web UI:</p> <ul> <li>http://127.0.0.1:9080</li> <li>http://127.0.0.1:9081</li> </ul> <p>All the metric data is ephemeral and will be lost with pod restarts. To uninstall Prometheus, run <code>make uninstall-prometheus</code></p> <p>Optionally, you can also install Grafana that will have the datasources configured and example dashboard ready using <code>make deploy-grafana</code></p>"},{"location":"CONTRIBUTING/#code-style","title":"Code style","text":"<p>k8gb project is using the coding style suggested by the Golang community. See the golang-style-doc for details.</p> <p>Please follow this style to make k8gb easy to review, maintain and develop. Run <code>make check</code> to automatically check if your code is compliant.</p>"},{"location":"CONTRIBUTING/#logging","title":"Logging","text":"<p>k8gb project is using the zerolog library for logging.</p> <ul> <li>Please make sure to follow the zerolog library concepts and conventions in the code.</li> <li>Try to use contextual logging whenever possible.</li> <li>Pay attention to error logging recommendations.</li> </ul>"},{"location":"CONTRIBUTING/#error-handling","title":"Error handling","text":"<p>See effective go errors first. Do not discard errors using <code>_</code> variables except tests, or, in truly exceptional situations. If a function returns an error, check it to make sure the function succeeded. If the function fails, consider logging the error and recording errors in metrics (see: logging recommendations).</p> <p>The following example demonstrates error handling inside the reconciliation loop: <pre><code>    var log = logging.Logger()\n\n    var m = metrics.Metrics()\n    ...\n    err = r.DNSProvider.CreateZoneDelegationForExternalDNS(gslb)\n    if err != nil {\n        log.Err(err).Msg(\"Unable to create zone delegation\")\n        m.ErrorIncrement(gslb)\n        return result.Requeue()\n    }\n</code></pre></p>"},{"location":"CONTRIBUTING/#commit-and-pull-request-message","title":"Commit and Pull Request message","text":"<p>We follow a rough convention for PR and commit messages, which is designed to answer two questions: what changed and why. The subject line should feature the what, and the body of the message should describe the why. The format can be described more formally as follows:</p> <pre><code>&lt;what was changed&gt;\n\n&lt;why this change was made&gt;\n\n&lt;footer&gt;\n</code></pre> <p>The first line is the subject and should be no longer than 70 characters. The second line is always blank. Consequent lines should be wrapped at 80 characters. This way, the message is easier to read on GitHub as well as in various git tools.</p> <pre><code>scripts: add the test-cluster command\n\nThis command uses \"k3d\" to set up a test cluster for debugging.\n\nFixes #38\n</code></pre> <p>Commit message can be made lightweight unless it is the only commit forming the PR. In that case, the message can follow the simplified convention:</p> <p><pre><code>&lt;what was changed and why&gt;\n</code></pre> This convention is useful when several minimalistic commit messages are going to form PR descriptions as bullet points of what was done during the final squash and merge for PR.</p>"},{"location":"CONTRIBUTING/#signature","title":"Signature","text":"<p>As a CNCF project, k8gb must comply with Developer Certificate of Origin (DCO) requirement. DCO GitHub Check automatically enforces DCO for all commits. Contributors are required to ensure that every commit message contains the following signature: <pre><code>Signed-off-by: NAME SURNAME &lt;email@address.example.org&gt;\n</code></pre> The best way to achieve this automatically for local development is to create the following alias in the <code>~/.gitconfig</code> file: <pre><code>[alias]\nci = commit -s\n</code></pre> When a commit is created in GitHub UI as a result of accepted suggested change, the signature should be manually added to the \"optional extended description\" field.</p>"},{"location":"CONTRIBUTING/#changelog","title":"Changelog","text":"<p>The CHANGELOG is automatically generated from Github PRs and Issues during release. Use dedicated keywords in PR message or manual PR and Issue linking for clean changelog generation. Issues and PRs should be also properly tagged with valid project tags (\"bug\", \"enhancement\", \"wontfix\", etc )</p>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>If contribution changes the existing APIs or user interface, it must include sufficient documentation explaining the use of the new or updated feature.</p>"},{"location":"CONTRIBUTING/#k8gbio-website","title":"k8gb.io website","text":"<p>k8gb.io website is a Jekyll-based static website generated from project markdown documentation and hosted by GitHub Pages. <code>gh-pages</code> branch contains the website source, including configuration, website layout, and styling. Markdown documents are automatically populated to <code>gh-pages</code> from the main branch and should be authored there. Changes to the k8gb.io website layout and styling should be checked out from the <code>gh-pages</code> branch and  PRs should be created against <code>gh-pages</code>.</p>"},{"location":"CONTRIBUTING/#local-website-authoring-and-testing","title":"Local website authoring and testing","text":"<p>These instructions will help you to set up and use local website authoring and testing environment:</p> <ul> <li>Check-out the <code>gh-pages</code> branch</li> <li>Create dedicated GitHub Personal Access Token with <code>public_repo</code> permission and assign it to the <code>JEKYLL_GITHUB_TOKEN</code> environment variable:</li> </ul> <pre><code>export JEKYLL_GITHUB_TOKEN=&lt;your-github-token&gt;\n</code></pre> <ul> <li>Run <code>make</code> and serve the local copy of the k8gb.io website.</li> </ul> <p>The target utilizes the <code>jekyll/jekyll</code> docker container to avoid local environment pollution from Jekyll gem dependencies.</p> <ul> <li>Open the <code>http://localhost:4000/</code> page in your browser.</li> <li>Website will get automatically rebuilt and refreshed in the browser to accommodate the related code changes.</li> </ul>"},{"location":"CONTRIBUTING/#using-custom-ssl-ca-certificate","title":"Using custom SSL CA certificate","text":"<p>When the dev environment is behind a proxy, it might be required to use a custom SSL CA certificate, otherwise, Jekyll is not able to update its plugins and dependencies. Local website authoring target automatically detects and wires the SSL certificate to its docker container from the path provided by the <code>CUSTOM_CERT_PATH</code> environment variable:</p> <pre><code>export CUSTOM_CERT_PATH=/path/to/custom/certificate.pem\n</code></pre> <p>NOTE: Certificate should be in a PEM format</p>"},{"location":"CONTRIBUTING/#env-support","title":".env support","text":"<p>Environment variables can be stored in a local <code>.env</code> file in the project's root to retain their persistency between the authoring sessions:</p> <pre><code>JEKYLL_GITHUB_TOKEN=&lt;your-github-token&gt;\nCUSTOM_CERT_PATH=&lt;your-custom-ssl-certificate&gt;\n</code></pre>"},{"location":"CONTRIBUTING/#end-to-end-demo-helper","title":"End-to-end demo helper","text":"<p>The demo helper is designed to work with <code>podinfo</code> that was deployed by</p> <pre><code>make deploy-test-apps\n</code></pre> <p>It will configure <code>podinfo</code> to expose geotag as part of an HTTP response.</p> <p>To test and/or demonstrate continuous query to GSLB enabled endpoint execute</p> <pre><code>make demo DEMO_URL=https://failover.test.exampledns.tk\n</code></pre> <p>The happy path will look like:</p> <pre><code>[Thu May 27 15:35:26 UTC 2021] ...\n\n200  \"message\": \"eu-west-1\",\n\n[Thu May 27 15:35:31 UTC 2021] ...\n\n200\n  \"message\": \"eu-west-1\",\n[Thu May 27 15:35:36 UTC 2021] ...\n</code></pre> <p>The sources for demo helper images can be found here</p> <p>To enable verbose debug output declare <code>DEMO_DEBUG=1</code> like <pre><code>make demo DEMO_URL=https://failover.test.exampledns.tk DEMO_DEBUG=1\n</code></pre></p>"},{"location":"CONTRIBUTING/#release-process","title":"Release process","text":"<ul> <li>Bump the version in <code>Chart.yaml</code>, see example PR. Make sure the commit message starts with <code>RELEASE:</code>.</li> <li>In addition, bump the version in <code>chart/k8gb/README.md</code> to reflect correct version tags in artifacthub after release, e.g. <pre><code>chart/k8gb/README.md:![Version: v0.13.0](https://img.shields.io/badge/Version-v0.13.0-informational?style=flat-square) ![Type: application](https://img.shields.io/badge/Type-application-informational?style=flat-square) ![AppVersion: v0.13.0](https://img.shields.io/badge/AppVersion-v0.13.0-informational?style=flat-square)\n</code></pre></li> <li>Ensure <code>stable</code> become <code>next</code>. Update <code>deploy/helm/stable.yaml</code> with <code>deploy/helm/next.yaml</code>.</li> <li>Merge the Pull Request after the review approval (make sure the squash or rebase is used, merge commit will not trigger the release pipeline)</li> <li> <p>At this point a DRAFT release will be created on GitHub. After the automatic tag &amp; release pipeline have been successfully completed, you check the release DRAFT and if it is OK, you click on the \"Publish release\" button.</p> </li> <li> <p>Check the helm publish pipeline status</p> </li> <li>Check the offline changelog status. This pipeline creates a pull request with an offline changelog. Do a review and if everything is ok, merge it.</li> </ul> <p>Congratulations, the release is complete!</p>"},{"location":"CONTRIBUTING/#signed-releases","title":"Signed releases","text":"<p>During the release process we generate also the provenance file that is compliant with https://in-toto.io/Statement/v0.1 schema. It contains the information about the github action run that was responsible for the release, but also other metadata about artifacts there were created and their signatures.</p> <p>This provenance file is signed itself and attached with the signature to the release artifacts. For signing the artifacts we use <code>cosign</code> tool and private key stored as the repository secret. Public key is available in the repository itself in file <code>cosign.pub</code>. This way anybody can verify the origin of arbitrary artifact. In order to regenerate the keys for cosign, one can run <code>cosign generate-key-pair</code>, use some passphrase and update the <code>COSIGN_{PRIVATE,PUBLIC}_KEY</code> &amp; <code>COSIGN_PASSWORD</code> repo secret and also the content of <code>cosign.pub</code> file.</p> <p>All the container images that are produced during the build are also signed with <code>cosign</code> and the signatures are also pushed to the container registries (dockerhub). So that users of k8gb can introduce OPA policy that imposes such verification on our images. These signatures are stored in OCI format under predictable name that can be found using <code>cosign triangulate $IMAGE</code> command. However, <code>cosign verify ..</code> with our public key should be sufficient.</p>"},{"location":"CONTRIBUTING/#software-bill-of-materials","title":"Software bill of materials","text":"<p>For each container image we also create Software bill of materials (SBOM) file + its signature that ends up as part of the release. These files follows this naming pattern: <code>k8gb_{version}_{os}_{arch}.tar.gz.sbom.json</code> and are generated using Syfttool.</p> <p>Thanks for contributing!</p>"},{"location":"address_discovery/","title":"Automatic CoreDNS exposed IP discovery","text":"<p>k8gb automatically discovers the correct IP addresses for DNS delegation during bootstrap.</p>"},{"location":"address_discovery/#how-address-discovery-works","title":"How Address Discovery Works","text":"<p>When k8gb needs to create DNS delegation records (delegated DNSEndpoints sitting in k8gb namespace), it needs to know CoreDNS IP addresses.</p> <p>- If you are running CoreDNS service as a LoadBalancer service (<code>.Values.coredns.serviceType</code>: <code>LoadBalancer</code>): k8gb automatically uses the external IP addresses of CoreDNS service type <code>LoadBalancer</code>.</p> <p>- If you are NOT running CoreDNS service as a LoadBalancer service (common for local development and testing): There are no external IP addresses. In this case, k8gb will use the address of the first Ingress labeled with <code>k8gb.io/ip-source=\"true\"</code>.</p> <p>This ensures that DNS records created by k8gb always point to the correct addresses, regardless of your environment or infrastructure setup.</p> <p>Note: k8gb fails on bootstrap if it cannot discover the address of the CoreDNS service or the first Ingress with the <code>k8gb.io/ip-source=\"true\"</code> label. This is to prevent misconfigurations that could lead to DNS resolution issues.</p>"},{"location":"address_discovery/#example-for-local-development","title":"Example for Local Development","text":"<p>If CoreDNS is not exposed as a LoadBalancer, label your Ingress to let k8gb know where to find the correct IP:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: k8gb-init-ingress\n  namespace: k8gb\n  labels:\n    k8gb.io/ip-source: \"true\"\nspec:\n...\n</code></pre>"},{"location":"admiralty/","title":"Integration with Admiralty","text":"<p>Combination of k8gb and admiralty.io provides powerful global multi-cluster capabilities.</p> <p>Admiralty will globally schedule. And k8gb will globally balance.</p> <p>This tutorial covers local end-to-end integration quick start of two projects.</p>"},{"location":"admiralty/#deploy-admiralty","title":"Deploy Admiralty","text":"<p>Just follow https://admiralty.io/docs/quick_start</p>"},{"location":"admiralty/#deploy-k8gb-to-target-clusters","title":"Deploy k8gb to Target clusters","text":"<pre><code>helm repo add k8gb https://www.k8gb.io\n\nkubectl --context kind-eu create ns k8gb\nhelm --kube-context kind-eu --namespace k8gb upgrade --install k8gb k8gb/k8gb --set k8gb.clusterGeoTag=eu --set k8gb.extGslbClustersGeoTags=us\n\nkubectl --context kind-us create ns k8gb\nhelm --kube-context kind-us --namespace k8gb upgrade --install k8gb k8gb/k8gb --set k8gb.clusterGeoTag=us --set k8gb.extGslbClustersGeoTags=eu\n</code></pre> <p>It will install k8gb instances to both Target clusters.</p> <p>Notice the GeoTag configuration.</p>"},{"location":"admiralty/#create-sample-workloads-on-the-source-cluster","title":"Create sample workloads on the source cluster","text":"<pre><code>helm --kube-context kind-cd upgrade --install podinfo podinfo/podinfo --set replicaCount=2 --set ingress.enabled=true\n</code></pre>"},{"location":"admiralty/#add-pod-annotations-to-enable-multi-cluster-scheduling","title":"Add pod annotations to enable multi-cluster scheduling","text":"<p>Edit <code>podinfo</code> Deployment to add Admiralty <code>multicluster.admiralty.io/elect</code> scheduling Annotation to Pod spec template</p> <pre><code>kubectl edit deploy podinfo\n</code></pre> <p>The Pod template spec part should look similar to</p> <pre><code>...\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: \"9898\"\n        prometheus.io/scrape: \"true\"\n        multicluster.admiralty.io/elect: \"\"\n      creationTimestamp: null\n...\n</code></pre> <p>Observe that pods were scheduled to Target cluster the application of Annotation patch.</p> <pre><code>kubectl --context kind-us get pod\nNAME                             READY   STATUS    RESTARTS   AGE\npodinfo-557c458ddb-9qpjf-l29w6   1/1     Running   0          11s\n\nkubectl --context kind-eu get pod\nNAME                             READY   STATUS    RESTARTS   AGE\npodinfo-557c458ddb-9nkmx-p7z9r   1/1     Running   0          15s\n</code></pre> <p>Observe that associated Ingress has also followed the Pods to the Target clusters</p> <pre><code>kubectl --context kind-eu get ing\nNAME      CLASS    HOSTS   ADDRESS   PORTS   AGE\npodinfo   &lt;none&gt;   *                 80      10m\n\nkubectl --context kind-eu get ing\nNAME      CLASS    HOSTS   ADDRESS   PORTS   AGE\npodinfo   &lt;none&gt;   *                 80      10m\n</code></pre>"},{"location":"admiralty/#add-k8gb-annotations-to-ingress-object-to-enable-global-load-balancing","title":"Add k8gb annotations to Ingress object to enable global load balancing","text":"<p>Observer that there are no Gslb resources in the Target clusters</p> <pre><code>kubectl --context kind-eu get gslb\nNo resources found in default namespace.\n\nkubectl --context kind-us get gslb\nNo resources found in default namespace.\n</code></pre> <p>Add k8gb strategy annotation to the Ingress object in the Source cluster</p> <pre><code>kubectl annotate ing podinfo k8gb.io/strategy=roundRobin\n</code></pre> <p>Observe that Gslb resources were properly created</p> <pre><code>kubectl --context kind-eu get gslb\nNAME      AGE\npodinfo   102s\n\nkubectl --context kind-us get gslb\nNAME      AGE\npodinfo   69s\n</code></pre> <p>Note: it's not a real working k8gb setup but just a way to demonstrate multi-cluster Gslb CR propagation with Admiralty.</p> <p>Please refer to https://www.k8gb.io/ documentation to create fully working real-life Gslb setup.</p>"},{"location":"components/","title":"K8GB Internal Components","text":"<p> The flow between internal k8gb components including scenario of Route53 integration.</p> <p> The flow between k8gb clusters including scenario of Route53 integration. </p>"},{"location":"deploy_azuredns/","title":"Azure DNS","text":"Using Azure Public DNS provider <p>This document outlines how to configure k8gb to use the Azure Public DNS provider. Azure Private DNS is not supported as it does not support NS records at this time. For private DNS scenarios in Azure, please refer to the Windows DNS documentation and consider implementing it using VM-based DNS services such as Windows DNS or BIND.</p>"},{"location":"deploy_azuredns/#external-dns-credentials-for-azure-dns","title":"external-dns credentials for Azure DNS","text":"<p>In this example, we will use a registered app in Microsoft Entra ID and it's corresponding Client ID / Client Secret to authenticate with the Azure DNS zone. All of the supported authentication fields supported by external-dns are supported by k8gb and can be used in the <code>azuredns</code> section of the <code>k8gb</code> Helm chart values.yaml file.</p>"},{"location":"deploy_azuredns/#sample-solution","title":"Sample solution","text":"<p>In this sample solution we will deploy two private AKS clusters in different regions. A workload will be deployed to both clusters and exposed to the internet with the help of k8gb and Azure Public DNS.</p>"},{"location":"deploy_azuredns/#reference-setup","title":"Reference Setup","text":"<p>The reference setup includes two private AKS clusters that can be deployed on two different regions for load balancing or to provide a failover solution.</p> <p>Configurable resources:</p> <ul> <li>Resource groups</li> <li>VNet and subnets</li> <li>Managed Identity</li> <li>Clusters</li> </ul>"},{"location":"deploy_azuredns/#run-the-sample","title":"Run the sample","text":"<ul> <li>To run the provided sample, please use the provided Makefile here.<ul> <li>Deploys all the required infrastructure and configurations</li> <li>Before executing, please fill all the local variables in the scripts with the correct naming for the resources in order to avoid having problems with your Azure policies</li> <li>Scripts will use Az CLI, please ensure that it is installed and logged when trying to execute the command<ul> <li>Microsoft Learn</li> </ul> </li> </ul> </li> </ul>"},{"location":"deploy_azuredns/#deploy-infrastructure","title":"Deploy infrastructure","text":"<p>This action will create resource groups, vnets and private AKS clusters to run all required workloads</p> <pre><code>make deploy-infra\n</code></pre>"},{"location":"deploy_azuredns/#setup-clusters","title":"Setup clusters","text":"<p>Install required Ingress controller in both clusters in order to deploy K8GB and demo application</p> <pre><code>make setup-clusters\n</code></pre>"},{"location":"deploy_azuredns/#install-k8gb","title":"Install K8gb","text":"<p>This action will install K8gb in both clusters using the provided sample values.yaml for each cluster. Please ensure that the are correctly updated before execution</p> <pre><code>make deploy-k8gb\n</code></pre>"},{"location":"deploy_azuredns/#install-demo-app","title":"Install demo app","text":"<p>Deploys the sample Podinfo workload with failover GLSB configured using annotations in the Ingress resource samples. Ensure that the hosts on the samples are correctly updated before execution</p> <pre><code>make deploy-demo\n</code></pre>"},{"location":"deploy_azuredns/#destroy-lab","title":"Destroy lab","text":"<ul> <li>Destroys the lab environment created for this sample</li> </ul> <pre><code>make destroy-infra\n</code></pre>"},{"location":"deploy_cloudflare/","title":"General deployment with Cloudflare integration","text":"<p>In this guide, we will demonstrate how to configure k8gb to integrate with Cloudflare for automated zone delegation configuration.</p>"},{"location":"deploy_cloudflare/#initial-setup","title":"Initial setup","text":"<p>As a prerequisite, we will need two Kubernetes clusters where you want to deploy k8gb and enable global load balancing between them.</p> <p>You can reuse local clusters from the Infoblox tutorial, the EKS-based setup from Route53 tutorial or any Kubernetes deployment method that is convenient to you.</p> <p>The specific Kubernetes deployment method is not essential for the focus of this documentation guide.</p> <p>For simplicity, we will assume that clusters have simple 'eu' and 'us' geotags.</p>"},{"location":"deploy_cloudflare/#deploy-k8gb-with-cloudflare-integration-enabled","title":"Deploy k8gb with Cloudflare integration enabled","text":"<p>Use <code>helm</code> to deploy a stable release from the Helm repo.</p> <pre><code>helm repo add k8gb https://www.k8gb.io\n</code></pre> <p>Example <code>values.yaml</code> configuration files can be found here</p> <p>Remember to change the zone-related values to point configuration to your own DNS zone.</p> <pre><code>k8gb:\n  dnsZones:\n  - parentZone: \"k8gb.io\"\n    loadBalancedZone: \"cloudflare-test.k8gb.io\"\n</code></pre>"},{"location":"deploy_cloudflare/#cloudflare-specific-configuration","title":"Cloudflare-specific configuration","text":"<p>Let's look closer at the Cloudflare section of the configuration examples.</p> <pre><code>cloudflare:\n  # -- Enable Cloudflare provider\n  enabled: true\n  # -- Cloudflare Zone ID\n  zoneID: cdebf92e613133e4bb176a14a9c5b730\n  # -- Configure how many DNS records to fetch per request\n  # see https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/cloudflare.md#throttling\n  dnsRecordsPerPage: 5000\n</code></pre> <p>Follow https://developers.cloudflare.com/fundamentals/setup/find-account-and-zone-ids/ to find your <code>zoneID</code></p>"},{"location":"deploy_cloudflare/#install-the-k8gb-helm-chart-in-each-cluster","title":"Install the k8gb helm chart in each cluster","text":"<p>In <code>eu</code> cluster, execute <pre><code>helm -n k8gb upgrade -i k8gb k8gb/k8gb --create-namespace -f ./docs/examples/cloudflare/k8gb-cluster-cloudflare-eu.yaml\n</code></pre></p> <p>In <code>us</code> cluster, execute <pre><code>helm -n k8gb upgrade -i k8gb k8gb/k8gb --create-namespace -f ./docs/examples/cloudflare/k8gb-cluster-cloudflare-us.yaml\n</code></pre></p>"},{"location":"deploy_cloudflare/#create-a-cloudflare-secret-in-each-cluster","title":"Create a Cloudflare secret in each cluster","text":"<pre><code>kubectl -n k8gb create secret generic cloudflare --from-literal=token=&lt;api-secret&gt;\n</code></pre> <p>Note: you can create Cloudflare API tokens at https://dash.cloudflare.com/profile/api-tokens</p>"},{"location":"deploy_cloudflare/#create-test-ingress-and-gslb-resources","title":"Create test Ingress and Gslb resources","text":"<p>Now we can test the setup with a pretty standard Gslb resource configuration.</p> <pre><code>apiVersion: k8gb.absa.oss/v1beta1\nkind: Gslb\nmetadata:\n  name: test-gslb-failover\n  namespace: test-gslb\nspec:\n  resourceRef:\n    apiVersion: networking.k8s.io/v1\n    kind: Ingress\n    matchLabels:\n      app: test-gslb-failover\n  strategy:\n    dnsTtlSeconds: 60 # Minimum for non-Enterprise Cloudflare https://developers.cloudflare.com/dns/manage-dns-records/reference/ttl/\n    primaryGeoTag: eu\n    type: failover\n</code></pre> <p>The only unusual thing here is <code>spec.strategy.dnsTtlSeconds</code> that should be of a minimum 60-second value in case you are operating a non-Enterprise Cloudflare subscription. The lower values will be rejected by Cloudflare API.</p> <p>Apply the Gslb and Ingress resources to each cluster.</p> <pre><code>kubectl apply -f ./docs/examples/cloudflare/test-gslb-failover.yaml\n</code></pre>"},{"location":"deploy_cloudflare/#check-zone-delegation-configuration","title":"Check Zone Delegation configuration","text":"<p>As a result of the setup, you should observe DNSEndpoint automatically created, similar to the one below:</p> <pre><code>$ kubectl -n k8gb get dnsendpoints.externaldns.k8s.io k8gb-ns-extdns -o yaml\napiVersion: externaldns.k8s.io/v1alpha1\nkind: DNSEndpoint\nmetadata:\n  labels:\n    k8gb.absa.oss/dnstype: extdns\n  creationTimestamp: \"2023-11-12T19:55:20Z\"\n  generation: 3\n  name: k8gb-ns-extdns\n  namespace: k8gb\n  resourceVersion: \"5851\"\n  uid: 5d240eb8-1c19-48c3-bf69-508545f52ea4\nspec:\n  endpoints:\n  - dnsName: cloudflare-test.k8gb.io\n    recordTTL: 60\n    recordType: NS\n    targets:\n    - gslb-ns-eu-cloudflare-test.k8gb.io\n    - gslb-ns-us-cloudflare-test.k8gb.io\n  - dnsName: gslb-ns-us-cloudflare-test.k8gb.io\n    recordTTL: 60\n    recordType: A\n    targets:\n    - 172.26.0.8\n    - 172.26.0.9\n</code></pre> <p>On the Cloudflare dashboard side, you should observe that NS and glue A records are automatically created:</p> <p></p>"},{"location":"deploy_cloudflare/#troubleshooting","title":"Troubleshooting","text":"<p>If something is not working as expected with the integration, check the logs of the externalDNS pod that is responsible for the creation of the DNS records with Cloudflare API.</p> <pre><code>kubectl -n k8gb logs -f deploy/external-dns\n</code></pre>"},{"location":"deploy_infoblox/","title":"General deployment with Infoblox integration","text":""},{"location":"deploy_infoblox/#prologue","title":"Prologue","text":"<p>For simplicity let's assume that you operate two geographically distributed clusters you want to enable global load-balancing for. In this example, two local clusters will represent those two distributed clusters.</p> <ul> <li> <p>Let's switch the context to the first cluster <pre><code>export KUBECONFIG=eu-cluster\n</code></pre></p> </li> <li> <p>Copy the default <code>values.yaml</code> from k8gb chart to any convenient location, e.g. <pre><code>cp chart/k8gb/values.yaml ~/k8gb/eu-cluster.yaml\n</code></pre></p> </li> <li> <p>Modify the example configuration. Important parameters described below:</p> </li> <li><code>dnsZone</code> - this zone will be delegated to the <code>edgeDNS</code> in your environment. E.g. <code>yourzone.edgedns.com</code></li> <li><code>edgeDNSZone</code> - this zone will be automatically configured by k8gb to delegate to <code>dnsZone</code> and will make k8gb controlled nodes act as authoritative server for this zone. E.g. <code>edgedns.com</code></li> <li><code>parentZoneDNSServers</code> stable DNS servers in your environment that is controlled by edgeDNS provider e.g. Infoblox so k8gb instances will be able to talk to each other through automatically created DNS names</li> <li><code>clusterGeoTag</code> to geographically tag your cluster. We are operating <code>eu</code> cluster in this example</li> <li><code>extGslbClustersGeoTags</code> contains Geo tag of the cluster(s) to talk with when k8gb is deployed to multiple clusters. Imagine your second cluster is <code>us</code> so we tag it accordingly</li> <li> <p><code>infoblox.enabled: true</code> to enable automated zone delegation configuration at edgeDNS provider. You don't need it for local testing and can optionally be skipped. Meanwhile, in this section we will cover a fully operational end-to-end scenario. The other parameters do not need to be modified unless you want to do something special. E.g. to use images from private registry</p> </li> <li> <p>Export Infoblox related information in the shell. <pre><code>export WAPI_USERNAME=&lt;WAPI_USERNAME&gt;\nexport WAPI_PASSWORD=&lt;WAPI_PASSWORD&gt;\n</code></pre></p> </li> <li> <p>Create the Infoblox secret which is used by k8gb to configure edgeDNS by running: <pre><code>kubectl create ns k8gb\nmake infoblox-secret\n</code></pre></p> </li> <li> <p>Expose associated k8gb CoreDNS service for DNS traffic on worker nodes.</p> <p>Check this document for detailed information.</p> </li> <li> <p>Let's deploy k8gb to the first cluster. Most of the helper commands are abstracted by GNU <code>make</code>. If you want to look under the hood please check the <code>Makefile</code>. In general, standard Kubernetes/Helm commands are used. Point deployment mechanism to your custom <code>values.yaml</code> <pre><code>make deploy-gslb-operator VALUES_YAML=~/k8gb/eu-cluster.yaml\n</code></pre></p> </li> <li> <p>It should deploy k8gb pretty quickly. Let's check the pod status <pre><code> kubectl -n k8gb get pod\nNAME                                                       READY   STATUS     RESTARTS   AGE\nk8gb-76cc56b55-t779s                                       1/1     Running    0          39s\nk8gb-coredns-799984c646-qz88m                              1/1     Running    0          41s\n</code></pre></p> </li> <li> <p>Deploy k8gb to the second cluster by repeating the same steps with the exception of:</p> </li> <li>Switch context to 2nd cluster with <code>export KUBECONFIG=us-cluster</code></li> <li>Create another custom <code>values.yaml</code> with <code>cp ~/k8gb/eu-cluster.yaml ~/k8gb/us-cluster.yaml</code></li> <li>Create another geo tag to enable cross cluster communication:<ul> <li><code>clusterGeoTag</code> becomes <code>us</code></li> <li><code>extGslbClustersGeoTags</code> becomes <code>eu</code></li> </ul> </li> <li> <p>Run the installation pointing to new values file <code>make deploy-gslb-operator VALUES_YAML=~/k8gb/us-cluster.yaml</code></p> </li> <li> <p>When your 2nd cluster is ready by checking with <code>kubectl -n k8gb get pod</code>, we can proceed with the sample application installation</p> </li> <li> <p>We will use well known testing community app of podinfo <pre><code>helm repo add podinfo https://stefanprodan.github.io/podinfo\nkubectl create ns test-gslb\nhelm upgrade --install podinfo --namespace test-gslb --set ui.message=\"us\" podinfo/podinfo\n</code></pre> As you can see above we did set special geo tag message in podinfo configuration matching cluster geo tag. It is just for demonstration purposes.</p> </li> <li> <p>Check that podinfo is running <pre><code>kubectl -n test-gslb get pod\nNAME                       READY   STATUS    RESTARTS   AGE\npodinfo-5cfcdc9c45-jbg96   1/1     Running   0          2m18s\n</code></pre></p> </li> <li> <p>Let's create Gslb CRD to enable global load balancing for this application. Notice the podinfo Service name <pre><code>kubectl -n test-gslb get svc\nNAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\npodinfo         ClusterIP   10.96.250.84    &lt;none&gt;        9898/TCP,9999/TCP   9m39s\n</code></pre></p> </li> <li> <p>Create a custom resource <code>~/k8gb/podinfogslb.yaml</code> describing an <code>Ingress</code> and a <code>Gslb</code> as per the sample below: <pre><code>---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: podinfo\n  namespace: test-gslb\n  labels:\n    app: podinfo\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: podinfo.cloud.example.com\n      http:\n        paths:\n        - path: /\n          backend:\n            service:\n              name: podinfo # This should point to Service name of testing application\n              port:\n                name: http\n---\napiVersion: k8gb.absa.oss/v1beta1\nkind: Gslb\nmetadata:\n  name: podinfo\n  namespace: test-gslb\nspec:\n  resourceRef:\n    apiVersion: networking.k8s.io/v1\n    kind: Ingress\n    matchLabels:\n      app: podinfo\n</code></pre></p> </li> <li> <p>And apply the resource in the target app namespace <pre><code>kubectl -n test-gslb apply -f podinfogslb.yaml\ngslb.k8gb.absa.oss/podinfo created\n</code></pre></p> </li> <li> <p>Check Gslb resource <pre><code>kubectl -n test-gslb get gslb\nNAME      AGE\npodinfo   39s\n</code></pre></p> </li> <li> <p>Check Gslb resource status <pre><code>kubectl -n test-gslb describe gslb\nName:         podinfo\nNamespace:    test-gslb\nLabels:       &lt;none&gt;\nAnnotations:  API Version:  k8gb.absa.oss/v1beta1\nKind:         Gslb\nMetadata:\n  Creation Timestamp:  2020-06-24T22:51:09Z\n  Finalizers:\n    k8gb.absa.oss/finalizer\n  Generation:        1\n  Resource Version:  14197\n  Self Link:         /apis/k8gb.absa.oss/v1beta1/namespaces/test-gslb/gslbs/podinfo\n  UID:               86d4121b-b870-434e-bd4d-fece681116f0\nSpec:\n  Ingress:\n    Rules:\n      Host:  podinfo.cloud.example.com\n      Http:\n        Paths:\n          Backend:\n            Service Name:  podinfo\n            Service Port:  http\n          Path:            /\n  Strategy:\n    Type:  roundRobin\nStatus:\n  Geo Tag:  us\n  Healthy Records:\n    podinfo.cloud.example.com:\n      172.17.0.10\n      172.17.0.7\n      172.17.0.8\n  Service Health:\n    podinfo.cloud.example.com:  Healthy\nEvents:                         &lt;none&gt;\n</code></pre></p> </li> <li> <p>In the output above you should see that Gslb detected the <code>Healthy</code> status of underlying <code>podinfo</code> standard Kubernetes Service</p> </li> <li> <p>Check that internal k8gb DNS servers are responding accordingly on this cluster</p> </li> <li>Pick one of the worker nodes to test with     <pre><code>k get nodes -o wide\nNAME                       STATUS   ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION     CONTAINER-RUNTIME\ntest-gslb2-control-plane   Ready    master   53m   v1.17.0   172.17.0.9    &lt;none&gt;        Ubuntu 19.10   4.19.76-linuxkit   containerd://1.3.2\ntest-gslb2-worker          Ready    &lt;none&gt;   52m   v1.17.0   172.17.0.8    &lt;none&gt;        Ubuntu 19.10   4.19.76-linuxkit   containerd://1.3.2\ntest-gslb2-worker2         Ready    &lt;none&gt;   52m   v1.17.0   172.17.0.7    &lt;none&gt;        Ubuntu 19.10   4.19.76-linuxkit   containerd://1.3.2\ntest-gslb2-worker3         Ready    &lt;none&gt;   52m   v1.17.0   172.17.0.10   &lt;none&gt;        Ubuntu 19.10   4.19.76-linuxkit   containerd://1.3.2\n</code></pre></li> <li>Use <code>dig</code> to make a DNS query to it     <pre><code>dig +short @172.17.0.10 podinfo.cloud.example.com\n172.17.0.8\n172.17.0.10\n172.17.0.7\n</code></pre></li> <li>One of your workers should already return DNS responses constructed by Gslb based on service health information</li> <li>If edgeDNS was configured you can query your standard infra DNS directly and it should return the same     <pre><code>dig +short podinfo.cloud.example.com\n172.17.0.8\n172.17.0.10\n172.17.0.7\n</code></pre></li> <li> <p>Now it's time to deploy this application to the first <code>eu</code> cluster. The steps and configuration are exactly the same. Just changing <code>ui.message</code> to <code>eu</code> <pre><code>kubectl create ns test-gslb\nhelm upgrade --install podinfo --namespace test-gslb --set ui.message=\"eu\" podinfo/podinfo\n</code></pre></p> </li> <li> <p>Apply exactly the same Gslb definition <pre><code>kubectl -n test-gslb apply -f podinfogslb.yaml\n</code></pre></p> </li> <li> <p>Check the Gslb resource status. <pre><code>k -n test-gslb describe gslb podinfo\nName:         podinfo\nNamespace:    test-gslb\nLabels:       &lt;none&gt;\nAnnotations:  API Version:  k8gb.absa.oss/v1beta1\nKind:         Gslb\nMetadata:\n  Creation Timestamp:  2020-06-24T23:25:08Z\n  Finalizers:\n    k8gb.absa.oss/finalizer\n  Generation:        1\n  Resource Version:  23881\n  Self Link:         /apis/k8gb.absa.oss/v1beta1/namespaces/test-gslb/gslbs/podinfo\n  UID:               a5ab509b-5ea2-49d6-982e-4129a8410c3e\nSpec:\n  Ingress:\n    Rules:\n      Host:  podinfo.cloud.example.com\n      Http:\n        Paths:\n          Backend:\n            Service Name:  podinfo\n            Service Port:  http\n          Path:            /\n  Strategy:\n    Type:  roundRobin\nStatus:\n  Geo Tag:  eu\n  Healthy Records:\n    podinfo.cloud.example.com:\n      172.17.0.3\n      172.17.0.5\n      172.17.0.6\n      172.17.0.8\n      172.17.0.10\n      172.17.0.7\n  Service Health:\n    podinfo.cloud.example.com:  Healthy\nEvents:                         &lt;none&gt;\n</code></pre></p> </li> <li> <p>Ideally you should already see that <code>Healthy Records</code> of <code>podinfo.cloud.example.com</code> return the records from both of the clusters. Otherwise, give it a couple of minutes to sync up.</p> </li> <li> <p>Now you can check the DNS responses the same way as before. <pre><code>dig +short podinfo.cloud.example.com\n172.17.0.8\n172.17.0.5\n172.17.0.10\n172.17.0.7\n172.17.0.6\n172.17.0.3\n</code></pre></p> </li> <li> <p>And for the final end-to-end test, we can use <code>curl</code> to query the application <pre><code>curl -s podinfo.example.com|grep message\n  \"message\": \"eu\",\n\ncurl -s podinfo.example.com|grep message\n  \"message\": \"us\",\n\ncurl -s podinfo.example.com|grep message\n  \"message\": \"us\",\n\ncurl -s podinfo.example.com||grep message\n  \"message\": \"eu\",\n</code></pre></p> </li> <li> <p>As you can see specially marked <code>podinfo</code> returns different geo tags showing us the Global Round Robin strategy is working as expected</p> </li> </ul> <p>Hope you enjoyed the ride!</p> <p>If anything unclear or is going wrong, feel free to contact us at https://github.com/k8gb-io/k8gb/issues. We will appreciate any feedback/bug report and Pull Requests are welcome.</p> <p>For more advanced technical documentation and fully automated local installation steps, see below.</p>"},{"location":"deploy_ns1/","title":"AWS based deployment with NS1 integration","text":"<p>Here we provide an example of k8gb deployment in AWS context with NS1 as edgeDNS provider</p>"},{"location":"deploy_ns1/#reference-setup","title":"Reference setup","text":"<p>Two EKS clusters in <code>eu-west-1</code> and <code>us-east-1</code>.</p> <p>The EKS setup is identical to Route53 tutorial</p> <p>Terraform code for cluster reference setup can be found here</p>"},{"location":"deploy_ns1/#deploy-k8gb","title":"Deploy k8gb","text":"<p>Use <code>helm</code> to deploy stable release from Helm repo</p> <pre><code>helm repo add k8gb https://www.k8gb.io\n</code></pre> <p>Example <code>values.yaml</code> configuration files can be found here</p> <p>In <code>eu-west-1</code> cluster execute <pre><code>helm -n k8gb upgrade -i k8gb k8gb/k8gb --create-namespace -f ./docs/examples/ns1/k8gb-cluster-ns1-eu-west-1.yaml\n</code></pre></p> <p>In <code>us-east-1</code> cluster execute <pre><code>helm -n k8gb upgrade -i k8gb k8gb/k8gb --create-namespace -f ./docs/examples/ns1/k8gb-cluster-ns1-us-east-1.yaml\n</code></pre></p> <p>Create NS1 secret in each cluster</p> <pre><code>export NS1_APIKEY=&lt;ns1-api-key&gt;\nmake ns1-secret\n</code></pre>"},{"location":"deploy_route53/","title":"AWS based deployment with Route53 integration","text":"<p>Here we provide an example of k8gb deployment in AWS context with Route53 as edgeDNS provider.</p>"},{"location":"deploy_route53/#reference-setup","title":"Reference Setup","text":"<p>Two EKS clusters in <code>eu-west-1</code> and <code>us-east-1</code>.</p> <p>Terraform code for cluster reference setup can be found here</p> <p>Feel free to reuse this code fully or partially and adapt for your existing scenario custom configuration like</p> <ul> <li>Existing VPC names</li> <li>Existing Public/private subnets tags</li> <li>EKS custom tags</li> <li>IRSA(IAM Roles for Service Accounts) role reference</li> </ul>"},{"location":"deploy_route53/#install-ingress-controller","title":"Install Ingress Controller","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.40.2/deploy/static/provider/aws/deploy.yaml\n</code></pre>"},{"location":"deploy_route53/#deploy-k8gb","title":"Deploy k8gb","text":"<p>Example helm configuration files can be found here</p> <p>Modify them to reflect your <code>dnsZone</code>, <code>edgeDNSZone</code>, valid <code>hostedZoneID</code> and <code>irsaRole</code> ARN.</p> <p>Clone k8gb repository and use <code>helm</code> with custom values</p> <pre><code>git clone https://github.com/k8gb-io/k8gb.git\ncd k8gb\n\nhelm repo add k8gb https://www.k8gb.io\nhelm repo update\n\n\n#switch kubectl context to eu-west-1\nhelm -n k8gb upgrade -i k8gb k8gb/k8gb --create-namespace -f ./docs/examples/route53/k8gb/k8gb-cluster-eu-west-1.yaml\n\n#switch kubectl context to us-east-1\nhelm -n k8gb upgrade -i k8gb k8gb/k8gb --create-namespace -f ./docs/examples/route53/k8gb/k8gb-cluster-us-east-1.yaml\n</code></pre>"},{"location":"deploy_route53/#test","title":"Test","text":"<p>Note: here and for all occurrences below whenever we speak about application to each cluster, we assume that you switch kubectl context and apply the same command to all clusters.</p> <ul> <li>Deploy test application to each cluster.</li> </ul> <pre><code>make deploy-test-apps\n</code></pre> <ul> <li> <p>Modify sample Gslb CR to reflect desired <code>.spec.ingress.rules[0].host</code> FQDN</p> </li> <li> <p>Apply Gslb CR to each cluster</p> </li> </ul> <pre><code>kubectl apply -f ./docs/examples/route53/k8gb/gslb-failover.yaml\n</code></pre> <ul> <li>Check Gslb status.</li> </ul> <pre><code>kubectl -n test-gslb get gslb test-gslb-failover -o yaml\n</code></pre> <ul> <li>Check route53 entries.</li> </ul> <pre><code>aws route53 list-resource-record-sets --hosted-zone-id $YOUR_HOSTED_ZONE_ID\n</code></pre> <p>You should see that <code>gslb-ns-$dnsZone-$geotag</code> NS and glue A records were created to automatically configure DNS zone delegation.</p> <ul> <li>Check test application availability.</li> </ul> <pre><code>curl -s failover.test.k8gb.io| grep message\n  \"message\": \"eu-west-1\",\n</code></pre> <p>Replace <code>failover.test.k8gb.io</code> with the domain you specified in Gslb spec.</p> <p>Notice that traffic was routed to <code>eu-west-1</code>.</p> <ul> <li>Emulate the failure in <code>eu-west-1</code></li> </ul> <pre><code>kubectl -n test-gslb scale deploy frontend-podinfo --replicas=0\n</code></pre> <ul> <li>Observe Gslb status change.</li> </ul> <pre><code>kubectl -n test-gslb get gslb test-gslb-failover -o yaml | grep status -A6\nstatus:\n  geoTag: us-east-1\n  healthyRecords:\n    failover.test.k8gb.io:\n    - 35.168.91.100\n  serviceHealth:\n    failover.test.k8gb.io: Healthy\n</code></pre> <p>IP in healthyRecords should change to the IP address of NLB in <code>us-east-1</code></p> <ul> <li>Check failover to <code>us-east-1</code></li> </ul> <pre><code>curl -s failover.test.k8gb.io| grep message\n  \"message\": \"us-east-1\",\n</code></pre> <p>Notice that traffic is properly failed over to <code>us-east-1</code></p> <ul> <li>Experiment</li> </ul> <p>Now you can scale <code>eu-west-1</code> back and observe that traffic is routed back to the primary cluster.</p> <p>In addition, you can test <code>roundRobin</code> load balancing strategy, which is spreading the traffic over the clusters in active-active mode.</p>"},{"location":"deploy_windowsdns/","title":"Windows DNS","text":"Using K8GB with a GSS-TSIG compatible DNS provider"},{"location":"deploy_windowsdns/#sample-solution-azure-based-private-deployment-with-windows-dns-integration","title":"Sample solution: Azure based private deployment with Windows DNS integration","text":"<p>In this sample solution we will create a common hub and spoke architecture with two private AKS clusters in different regions. The same pattern can be used with any other Kubernetes distribution and any other DNS provider that supports GSS-TSIG.</p> <p>Here we provide an example of k8gb deployment in Azure environment with Windows DNS as edgeDNS provider.</p>"},{"location":"deploy_windowsdns/#reference-setup","title":"Reference Setup","text":"<p>The reference setup includes two private AKS clusters that can be deployed on two different regions for load balancing or to provide a failover solution.</p> <p></p> <p>The solution design can be found here.</p> <p>Configurable resources:</p> <ul> <li>Resource groups</li> <li>VNet and subnets</li> <li>Peering</li> <li>Managed Identity</li> <li>Clusters</li> </ul>"},{"location":"deploy_windowsdns/#run-the-sample","title":"Run the sample","text":"<ul> <li> <p>This lab requires a running AD Domain Controller with DNS and KDC services working</p> <ul> <li>There are several tutorials available online, but this Microsoft Learn article will probably help you out </li> <li>Microsoft Learn</li> </ul> </li> <li> <p>To run the provided sample, please use the provided Makefile here.</p> <ul> <li>Deploys all the required infrastructure and configurations</li> <li>Before executing, please fill all the local variables in the scripts with the correct naming for the resources in order to avoid having problems with your Azure policies</li> <li>Scripts will use Az CLI, please ensure that it is installed and logged when trying to execute the command<ul> <li>Microsoft Learn</li> </ul> </li> </ul> </li> </ul>"},{"location":"deploy_windowsdns/#deploy-infrastructure","title":"Deploy infrastructure","text":"<p>This action will create resource groups, vnets, peering between vnets and private AKS clusters to run all required workloads</p> <pre><code>make deploy-infra\n</code></pre>"},{"location":"deploy_windowsdns/#setup-clusters","title":"Setup clusters","text":"<p>Install required Ingress controller in both clusters in order to deploy K8GB and demo application</p> <pre><code>make setup-clusters\n</code></pre>"},{"location":"deploy_windowsdns/#configure-gss-tsig-authentication-for-dns-updates","title":"Configure GSS-TSIG authentication for DNS updates","text":"<p>Before deploying K8GB and the demo workload, ensure required configurations on Windows DNS</p>"},{"location":"deploy_windowsdns/#domain-controller-config","title":"Domain Controller config","text":"<ul> <li>Ensure that the Network Security is configured only for AES256</li> </ul> <ul> <li>Ensure that the DNS Zone has only Secure updates option enabled</li> </ul> <ul> <li>Ensure that the DNS Zone has the option \"Allow zone transfers\" check with the option \"To any server\" under the tab Zone Transfers on the zone properties</li> </ul> <ul> <li>Create a new Active Directory user<ul> <li>The user should be created with \"Encryptions options\" for Kerberos AES256 encryption</li> <li>The user needs to be added to the DNSAdmin group, or,</li> <li>Select the zone that will have dynamic updates in DNS Manager, right click and select Properties. Under the Security tab, add the created user and add the permissions Write, Create all child objects and Delete all child objects</li> </ul> </li> </ul>"},{"location":"deploy_windowsdns/#k8gb-externaldns-configuration","title":"K8GB / ExternalDNS configuration","text":"<ul> <li>ExternalDNS configuration<ul> <li>For communication with WindowsDNS, ExternalDNS should be configured with the RFC2136 provider with GSS-TSIG option</li> <li>External DNS - RFC2126</li> <li>A sample values.yaml for K8GB configuration can be found here.<ul> <li>Ensure that the following properties are updated with your values:<ul> <li>dnsZone</li> <li>edgeDNSZone</li> <li>parentZoneDNSServers</li> <li>host - always use FQDN with GSS-TSIG, not IP address</li> <li>kerberos-username</li> <li>kerberos-password</li> <li>kerberos-realm</li> </ul> </li> </ul> </li> <li>At this moment ExternalDNS doesn't provide a way to use secrets as the source for the kerberos-password setting, so you must ensure this is stored in a secure way</li> </ul> </li> </ul> <pre><code>rfc2136:\n  enabled: true\n  rfc2136Opts:\n    - host: AD-DC.k8gb.local #when using gssTsig, use the FQDN of the host, not an IP\n    - port: 53\n  rfc2136auth:\n    insecure: \n      enabled: false\n    tsig:\n      enabled: false\n      tsigCreds:\n        - tsig-secret-alg: hmac-sha256\n        - tsig-keyname: externaldns-key\n    gssTsig:\n      enabled: true\n      gssTsigCreds:\n        - kerberos-username: ad-user-account\n        - kerberos-password: ad-user-account-password\n        - kerberos-realm: cloud.lab\n</code></pre>"},{"location":"deploy_windowsdns/#install-k8gb","title":"Install K8gb","text":"<p>This action will install K8gb in both clusters using the provided sample values.yaml for each cluster. Please ensure that the are correctly updated before execution</p> <pre><code>make deploy-k8gb\n</code></pre>"},{"location":"deploy_windowsdns/#install-demo-app","title":"Install demo app","text":"<p>Deploys the sample Podinfo workload with failover GLSB configured using annotations in the Ingress resource samples. Ensure that the hosts on the samples are correctly updated before execution</p> <pre><code>make deploy-demo\n</code></pre>"},{"location":"deploy_windowsdns/#destroy-lab","title":"Destroy lab","text":"<ul> <li>Destroys the lab environment created for this sample</li> </ul> <pre><code>make destroy-infra\n</code></pre>"},{"location":"dynamic_geotags/","title":"Dynamic GeoTags","text":"<p>From v0.15.0, k8gb makes it easier to configure and manage cluster GeoTags.</p>"},{"location":"dynamic_geotags/#what-is-a-geotag","title":"What is a GeoTag?","text":"<p>A GeoTag is a short identifier (for example: <code>eu</code>, <code>us</code>, <code>za</code>) that uniquely marks each k8gb cluster\u2019s location or role. GeoTags are essential for k8gb\u2019s global DNS-based routing and failover logic.</p>"},{"location":"dynamic_geotags/#how-to-configure-geotags","title":"How to configure GeoTags","text":"<p>You configure GeoTags directly in your values.yaml (usually when installing or upgrading k8gb via Helm):</p> <p><pre><code>k8gb:\n  # Unique GeoTag for this cluster. Common values: \"eu\", \"us\", \"za\"\n  clusterGeoTag: \"eu\"\n\n  # Comma-separated list of GeoTags for all other k8gb clusters in your GSLB network.\n  # Example: \"eu,us,za\"\n  extGslbClustersGeoTags: \"eu,us\"\n</code></pre> - <code>clusterGeoTag</code>: This value must be unique for each cluster. It identifies this k8gb instance. - <code>extGslbClustersGeoTags</code>: This is a comma-separated list of all external k8gb clusters\u2019 GeoTags.</p> <p>Both values are set in values.yaml and are automatically passed to the k8gb Pod as environment variables by the Helm chart.</p>"},{"location":"dynamic_geotags/#whats-new-with-dynamic-geotags","title":"What\u2019s new with Dynamic GeoTags?","text":"<p>Previously, any change in the list of external clusters (extGslbClustersGeoTags) required you to update and restart all k8gb pods across all clusters, which was inconvenient and error prone especially as the number of clusters grew.</p> <p>Dynamic GeoTags allow k8gb to discover external GeoTags directly from DNS (from NS records on the parent zone), without the need to keep all values manually in sync.</p> <p>If the <code>extGslbClustersGeoTags</code> value is empty, k8gb will attempt to extract external GeoTags dynamically at runtime.</p> <ul> <li>This reduces manual configuration and operational overhead.</li> <li>You can add or remove clusters without having to update and restart all existing k8gb instances.</li> <li>It\u2019s especially useful for dynamic, cloud-native, or large-scale multi-cluster environments.</li> </ul>"},{"location":"dynamic_geotags/#example-valuesyaml","title":"Example (<code>values.yaml</code>):","text":"<pre><code>k8gb:\n  clusterGeoTag: \"eu\"\n  extGslbClustersGeoTags: \"\"  # leave empty to enable dynamic discovery\n</code></pre>"},{"location":"dynamic_geotags/#important-considerations","title":"Important considerations","text":"<p>Dynamic GeoTags provide convenience and flexibility, but it\u2019s important to understand their impact on your DNS infrastructure:</p> <p>\u26a0\ufe0f WARNING: Enabling dynamic GeoTags adds two extra DNS queries per reconciliation for each GSLB resource. In most cases, this overhead is negligible, but with a large number of GSLBs and short reconciliation intervals, your DNS server could become overwhelmed.</p> <p>If you experience high DNS query load or see signs of DNS server saturation, you can mitigate the issue as follows:</p> <p>1. Increase the number of DNS servers: Add more DNS servers to the list <code>.Values.k8gb.edgeDNSServers</code> in your <code>values.yaml</code>. k8gb will choose one DNS server at random (round-robin) for each reconciliation, distributing the load.</p> <p>2. Increase the reconciliation interval Raise the value of <code>.Values.k8gb.reconcileRequeueSeconds</code> in your <code>values.yaml</code>. By increasing this interval, you reduce how often k8gb triggers reconciliations, which directly decreases the DNS query rate. You can also set it to <code>0</code>\u2014in this case, reconciliation will only occur in response to changes in GSLB, Ingress, DNSEndpoint, or during initial bootstrap.</p> <p>3. Revert to static external GeoTags If dynamic GeoTags are not suitable for your environment, you can switch back to using the <code>.Values.k8gb.extGslbClustersGeoTags</code> to explicitly define the list of remote cluster tags, disabling dynamic discovery.</p> <p>In summary:  - Dynamic GeoTags simplify configuration but come with extra DNS queries per GSLB.   - For most environments, this is not an issue.   - For very large-scale or highly sensitive environments, use the mitigations above to prevent DNS overload.</p>"},{"location":"exposing_dns/","title":"Exposing DNS UDP traffic for k8gb","text":"<p>In order for k8gb to function properly, associated CoreDNS service deployed with k8gb needs to be exposed for external DNS UDP traffic on cluster worker nodes.</p> <p>Actual ways to achieve this depend on many factors, such as underlying infrastructure (cloud, on-prem, managed vs bare-metal setup), means to expose CoreDNS service (ClusterIP, LoadBalancer), type of load balancer or ingress controller used etc. This topic is outside the project's scope, as often the related configuration is shared by cluster services, requires additional permissions, and as result can't be owned by k8gb controller deployment. However, we can describe a few examples using common Kubernetes configurations, which have been thoroughly tested in local and production environments.</p>"},{"location":"exposing_dns/#ingress-controller-with-udp-support-nginx","title":"Ingress Controller with UDP support (NGINX)","text":"<p>Check NGINX Ingress controller official documentation for additional information</p> <p>In general, an Ingress resource doesn't support TCP or UDP services. In order to let NGINX Ingress controller know that we want to expose UDP port for k8gb CoreDNS service (<code>k8gb-coredns</code>), we need to create or patch <code>udp-services</code> ConfigMap in a namespace where NGINX ingress controller is installed (<code>ingress-nginx</code> by default). Its <code>data</code> section would contain UDP 53 port mapping for CoreDNS service deployed with k8gb chart release.</p> <p>Associated CoreDNS service can be found in the same namespace where k8gb chart release is deployed. Service name is prefixed with chart release name.</p> <p>Example <code>udp-services</code> ConfigMap manifest: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: udp-services\n  namespace: ingress-nginx\ndata:\n  53: \"k8gb/k8gb-coredns:53\"  # &lt;== \"&lt;K8GB_DEPLOYMENT_NAMESPACE&gt;/&lt;K8GB_CHART_RELEASE&gt;-coredns\"\n</code></pre></p> <p>It can be also created or patched by running <code>kubectl</code> one-liner:</p> <pre><code># Patch the existing `udp-services` ConfigMap in NGINX Ingress controller namespace:\n\nkubectl patch -n ingress-nginx -p '{\"data\":{\"53\":\"k8gb/k8gb-coredns:53\"}}' --type=merge cm/udp-services\n\n# Or create `udp-services` ConfigMap if it doesn't exist, e.g.:\n\nkubectl create -n ingress-nginx cm udp-services --from-literal=\"53\"=\"k8gb/k8gb-coredns:53\"\n</code></pre> <p>Local project setup does this patching automatically.</p>"},{"location":"exposing_dns/#coredns-service-lookup","title":"CoreDNS service lookup","text":"<p>k8gb is trying to find a service annotated with <code>app.kubernetes.io/name=coredns</code> within the same namespace where controller itself is deployed into. If the service has <code>Status.LoadBalancer.Ingress[0].Hostname</code>, k8gb will use the resolved IPs to reach CoreDNS on this cluster.</p>"},{"location":"exposing_dns/#external-load-balancer","title":"External load balancer","text":"<p>CoreDNS can be also exposed for DNS UDP traffic via external load balancer, if underlying infrastructure supports that. AWS EKS with NLB and k3d with ServiceLB are good examples of such an infrastructure proven to work for k8gb deployments. We're using this approach in our AWS+Route53 reference setup with k8gb helm chart providing out-of-the-box support for external load balancer scenario. CoreDNS service is configured by setting <code>coredns.serviceType</code> helm chart value to <code>LoadBalancer</code>: <pre><code># k8gb helm chart values.yaml example:\n\ncoredns:\n  ...\n  serviceType: LoadBalancer # &lt;== expose UDP DNS traffic via external load balancer\n  service:\n    annotations:\n       service.beta.kubernetes.io/aws-load-balancer-type: nlb\n</code></pre></p> <p>In general, resulting <code>Service</code> resource configuration for k8gb CoreDNS looks like:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: nlb # &lt;== tell AWS to use NLB load balancer\n  name: k8gb-coredns\n  namespace: k8gb\nspec:\n  ports:\n  - name: udp-53 # specify DNS UDP port (53)\n    port: 53\n    protocol: UDP\n  selector:\n    app.kubernetes.io/instance: k8gb\n    app.kubernetes.io/name: coredns\n  type: LoadBalancer # &lt;== set service type to LoadBalancer\n</code></pre>"},{"location":"ingress_annotations/","title":"Ingress Annotations","text":"<p>Instead of direct Gslb resource creation there is ability to enable global load balancing by setting annotations on the standard Ingress objects.</p> Annotation Description Type k8gb.io/strategy Glsb strategy \"<code>roundRobin</code>\" | \"<code>failover</code>\" k8gb.io/primary-geotag Arbitrary geotag string (e.g. \"<code>eu</code>\")"},{"location":"intro/","title":"K8GB","text":"<p>A Global Server Load Balancing solution with a focus on having cloud native qualities and work natively in a Kubernetes context. The term GSLB, as used in this context, obeys the same principles as defined in the following sources:</p> <ul> <li>What Is GSLB? Load Balancing Explained - Cloudflare</li> <li>Global Server Load Balancing Definition - AVI Networks</li> <li>What is Global Server Load Balancing (GSLB)? - A10 Networks</li> <li>Global Load Balancing, Caching and TTLs - NS1 - Important points on DNS Caching and how it affects GSLB</li> </ul> <p>In short, the ability to direct HTTP requests to a local load balancer (Kubernetes Ingress controller instances) based on the health of services (Pods) in multiple, potentially geographically dispersed, Kubernetes clusters whether on premises or in cloud. With additional options around what criteria to use (round robin, weighting, active/passive, etc.) when determining the best local load balancer/ingress instance to resolve.</p>"},{"location":"intro/#motivation","title":"Motivation","text":"<p>The ability to load balance HTTP requests across multiple Kubernetes clusters, running in multiple data centers/clouds is a key requirement for a resilient system. At the time of writing there does not seem to be an existing OSS GSLB (Global Server Load Balancer) solution that will support this requirement in a cloud native, Kubernetes friendly way.</p>"},{"location":"intro/#service-health","title":"Service health","text":"<p>One important aspect of this solution is that the GSLB load balancing should be based on the availability of Kubernetes Pods that a GSLB enabled host represents. I.e. Pod health, as determined by the configured liveness probes and readiness probes of a Pod should determine which resulting IP of the downstream Ingress will be resolved.</p> <p>This is in contrast to most existing OSS GSLB solutions which focus on traditional ICMP, TCP and HTTP health checks.</p>"},{"location":"intro/#goals","title":"Goals","text":"<p>The goal of this project is to provide an implementation of a cloud native GSLB that meets the following requirements:</p> <ul> <li>Is lightweight in terms of resource requirements and runtime complexity</li> <li>Runs well in a Kubernetes cluster</li> <li>To achieve high availability/redundancy we should be able to run multiple instances (potentially across multiple data centers or clouds) with shared state</li> <li>Use other proven, supported (CNCF projects as an example) and popular open source projects where applicable. Don't reinvent the wheel where not necessary</li> <li>Allow end users to define their GSLB configuration via Kubernetes native means (resource annotations, CRD's etc.)</li> <li>Provide observability into the operational health of the solution</li> </ul>"},{"location":"intro/#use-cases","title":"Use cases","text":"<p>The following outlines some common use cases and what this solution should solve for.</p>"},{"location":"intro/#1-basic-single-cluster","title":"1. Basic Single cluster","text":"<p>At it's simplest, an HTTP request should be handled by a healthy service. This service might reside in multiple Kubernetes clusters, all of which might be in geographically disparate locations, including a/any cloud provider/s.</p> <p></p>"},{"location":"intro/#11-application-team","title":"1.1 Application team","text":"<p>In the use case above, the following resources are configured by the application team:</p> <p>A. The Kubernetes <code>Gslb</code> CRD (Custom Resource Definition) is created which indicates to an k8gb controller that it should create the necessary GSLB configuration for the cluster.</p> <p>A potential example of what this <code>Gslb</code> resource would look like:</p> <pre><code>apiVersion: k8gb.absa.oss/v1beta1\nkind: Gslb\nmetadata:\n  name: app\nspec:\n  resourceRef:\n    apiVersion: networking.k8s.io/v1\n    kind: Ingress\n    matchLabels:\n      app: app\n  strategy: roundRobin # Use a round robin load balancing strategy, when deciding which downstream clusters to route clients too\n</code></pre> <p>On creating this <code>Gslb</code> resource, the k8gb controller watching the cluster where this resource is created, will:</p> <ol> <li>Lookup an <code>Ingress</code> resource that allow requests with the GSLB host (<code>app.cloud.example.com</code>). The ingress is handled by the cluster's Ingress controller</li> <li>Configure a health check strategy on the underlying <code>app</code> Pods. The Pods here are the Pods matched by the Service configured by <code>service.name</code></li> <li>Based on the health (see Service health) of those Pods, if at least one of the Pods is healthy, add DNS records with the external addresses of the cluster's nodes running the Ingress controllers</li> </ol>"},{"location":"intro/#12-client","title":"1.2 Client","text":"<p>In the use case above, the following would describe a client request:</p> <ol> <li>Client makes a request to https://app.cloud.example.com</li> <li>In resolving the IP for <code>app.cloud.example.com</code>, the Recursive Resolver forwards the requests to one of the instances of k8gb</li> <li>One of the cluster Ingress node IPs is returned to the client. E.g. <code>10.0.100.20</code></li> <li>The client, using the resolved IP of <code>10.0.100.20</code> now makes a connection and proceeds with the request. The request will be handled by one of the cluster's Ingress controllers and via the created GSLB Ingress resource, the request is proxied through to one of the available Pods as per the usual Kubernetes Ingress mechanics</li> </ol>"},{"location":"intro/#13-outcome","title":"1.3 Outcome","text":"<p>In this use case, only Kubernetes cluster X would be eligible to handle ingress traffic for <code>https://app.cloud.example.com</code> as there was no <code>Gslb</code> resource created in Kubernetes cluster Y.</p>"},{"location":"intro/#2-basic-multi-cluster","title":"2. Basic Multi cluster","text":"<p>In this use case, we create a second <code>Gslb</code> resource in Kubernetes cluster Y making both cluster X and Y eligible to handle ingress traffic. However, this use case should apply to any amount of clusters as well.</p> <p></p>"},{"location":"intro/#21-application-team","title":"2.1 Application team","text":"<p>In this use case the same steps for Application team are executed but on Kubernetes cluster Y. This means that the k8gb instance for cluster Y (assuming healthy Pods) will have added the Ingress node's external IP addresses.</p> <p>This means that from an overall k8gb perspective, there are now external IPs for both clusters, X and Y. This implies that all k8gb instances share common state and contain IPs for all eligible ingress nodes across all clusters. This enables any instance of k8gb to handle resolution for <code>Gslb</code> resource <code>hosts</code>'s.</p>"},{"location":"intro/#22-client","title":"2.2 Client","text":"<p>Once again, the client request is handled much the same as the first use case, except for the fact that the cluster ingress IP resolved will use a round robin strategy (the default strategy) between clusters X and Y.</p> <ol> <li>Same as basic use case</li> <li>Same as basic use case</li> <li>One of the cluster Ingress node IPs from cluster X is returned to the client. E.g. <code>10.0.100.20</code></li> <li>The client, using the resolved IP of <code>10.0.100.20</code> now makes a connection and proceeds with the request</li> <li>On the next request, one of the cluster Ingress node IPs from cluster Y is returned to the client. E.g. <code>10.0.200.40</code></li> <li>The client, using the resolved IP of <code>10.0.200.40</code> now makes a connection and proceeds with the request</li> </ol>"},{"location":"intro/#23-outcome","title":"2.3 Outcome","text":"<p>This use case demonstrates that clusters with healthy Pods should have their Ingress node IPs eligible for resolution, across all clusters configured with a <code>Gslb</code> resource with the same <code>spec.name</code>.</p> <p>The load balancing strategy should be configurable, see Load balancing strategies</p>"},{"location":"intro/#3-unhealthy-service-multi-cluster","title":"3. Unhealthy service - Multi cluster","text":"<p>This use case demonstrates what should happen if the Pods in a cluster (in this use case, cluster Y) are not available or unhealthy.</p> <p></p>"},{"location":"intro/#31-application-team","title":"3.1 Application team","text":"<p>Same as the multi cluster use case.</p>"},{"location":"intro/#32-client","title":"3.2 Client","text":"<ol> <li>Same as basic use case</li> <li>Same as basic use case</li> <li>Based on criteria, cluster Ingress node IPs from cluster X are returned (<code>10.0.100.20</code>), given that there are no healthy Pods in cluster Y and therefore those cluster Ingress node IPs for cluster Y have been removed</li> <li>The client, using the resolved IP of <code>10.0.100.20</code> now makes a connection and proceeds with the request</li> <li>On the next request, another one of the cluster Ingress node's IPs from cluster X is returned to the client. E.g. <code>10.0.100.21</code></li> <li>The client, using the resolved IP of <code>10.0.100.21</code> now makes a connection and proceeds with the request</li> </ol>"},{"location":"intro/#33-outcome","title":"3.3 Outcome","text":"<p>This use case demonstrates that clusters with no healthy Pods should not have their Ingress node IPs eligible for resolution. Meaning that no ingress traffic should ever be sent to clusters where the application is not in a state to accept requests.</p> <p>If the Pods in cluster Y were to once again become healthy (liveness and readiness probes start passing) then the Ingress node IPs for cluster Y would once again be added to the eligible pool of Ingress node IPs.</p>"},{"location":"intro/#load-balancing-strategies","title":"Load balancing strategies","text":"<p>The following load balancing strategies, as it relates to resolving Ingress node IPs, should be provided as part of the initial implementation: * roundRobin * weightRoundRobin * failover * geoip</p> <p>see strategies for details.</p> <p>The above strategies are specified as part of the <code>Gslb</code> resource(s) <code>spec</code>.</p>"},{"location":"intro/#configuration","title":"Configuration","text":"<p><code>Gslb</code> resources should contain all configuration options for the GSLB hosts they represent. However, any other global k8gb specific configuration should be specified as arguments to the binary or by reading a specified YAML configuration file.</p>"},{"location":"intro/#runtime-environments","title":"Runtime environments","text":"<p>k8gb instances are deployed to Kubernetes target clusters, next to GSLB-enabled workloads. Zero control clusters required.</p>"},{"location":"intro/#existing-gslb-projects","title":"Existing GSLB projects","text":"<p>The following projects represent examples of other GSLB implementations that could be leveraged or used as reference. However, it is important that the implementation of this project adhere to the goals outlined and which may not align with the implementation of these projects.</p>"},{"location":"intro/#open-source","title":"Open source","text":"<ul> <li>Polaris - https://github.com/polaris-gslb/polaris-gslb</li> <li>PowerGSLB - https://github.com/AlekseyChudov/powergslb</li> <li>https://github.com/datianshi/opensource-gslb</li> </ul>"},{"location":"intro/#commercial","title":"Commercial","text":"<ul> <li>AVI Networks - https://avinetworks.com/glossary/global-server-load-balancing-2/</li> <li>F5 Networks - https://www.f5.com/products/global-server-load-balancing-gslb</li> <li>See https://blog.openshift.com/deploying-openshift-applications-multiple-datacenters/ (Networking section)</li> <li>Infoblox DTC - https://www.infoblox.com/products/dns-traffic-control/</li> </ul>"},{"location":"liqo/","title":"Integration with Liqo","text":"<p>You can provide powerful global multi-cluster capabilities by combining k8gb and liqo.io.</p> <p>In this tutorial, you will learn how to leverage Liqo and K8GB to deploy and expose a multi-cluster application through a global ingress. More in detail, this enables improved load balancing and distribution of the external traffic towards the application replicated across multiple clusters.</p> <p>Liqo will globally schedule workloads and provide east-west connectivity, while K8GB will globally balance user traffic providing north-south connectivity over the multi-cluster and/or multi-provider environment.</p> <p>The figure below outlines the high-level scenario, with a client consuming an application from either cluster 1 (e.g., located in EU) or cluster 2 (e.g., located in the US), based on the endpoint returned by the DNS server.</p> <p></p>"},{"location":"liqo/#setup-environment","title":"Setup Environment","text":"<p>Checkout the liqo docs to get the environment setup script and to get more details. It creates the k3d clusters required for the K8GB playground as described in Local playground for testing and development and installs Liqo over them.</p>"},{"location":"liqo/#peer-the-clusters","title":"Peer the clusters","text":"<p>To proceed, first generate a new peer command from the gslb-us cluster:</p> <pre><code>PEER_US=$(liqoctl generate peer-command --only-command --kubeconfig $KUBECONFIG_US)\n</code></pre> <p>And then, run the generated command from the gslb-eu cluster:</p> <pre><code>echo \"$PEER_US\" | bash\n</code></pre>"},{"location":"liqo/#deploy-an-application","title":"Deploy an application","text":"<p>First, create a hosting namespace in the gslb-eu cluster, and offload it to the remote cluster through Liqo.</p> <pre><code>kubectl create namespace podinfo\nliqoctl offload namespace podinfo --namespace-mapping-strategy EnforceSameName\n</code></pre> <p>At this point, it is possible to deploy the podinfo helm chart in the <code>podinfo</code> namespace:</p> <pre><code>helm upgrade --install podinfo --namespace podinfo podinfo/podinfo \\\n    -f https://raw.githubusercontent.com/liqotech/liqo/master/examples/global-ingress/manifests/values/podinfo.yaml\n</code></pre> <p>This chart creates a Deployment with a custom affinity to ensure that the two frontend replicas are scheduled on different nodes and clusters:</p> <pre><code>affinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: node-role.kubernetes.io/control-plane\n          operator: DoesNotExist\n  podAntiAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n    - labelSelector:\n        matchExpressions:\n        - key: app.kubernetes.io/name\n          operator: In\n          values:\n          - podinfo\n      topologyKey: \"kubernetes.io/hostname\"\n</code></pre> <p>Additionally, it creates an Ingress resource configured with the ingress_annotations.mdThis annotation will instruct the K8GB Global Ingress Controller to distribute the traffic across the different clusters. ingress_annotations.mdis an HTTP service, you can contact it using the curl command. Use the <code>-v</code> option to understand which of the nodes is being targeted.</p> <p>You need to use the DNS server in order to resolve the hostname to the IP address of the service. To this end, create a pod in one of the clusters (it does not matter which one) overriding its DNS configuration.</p> <pre><code>HOSTNAME=\"liqo.cloud.example.com\"\nK8GB_COREDNS_IP=$(kubectl get svc k8gb-coredns -n k8gb -o custom-columns='IP:spec.clusterIP' --no-headers)\n\nkubectl run -it --rm curl --restart=Never --image=curlimages/curl:7.82.0 --command \\\n    --overrides \"{\\\"spec\\\":{\\\"dnsConfig\\\":{\\\"nameservers\\\":[\\\"${K8GB_COREDNS_IP}\\\"]},\\\"dnsPolicy\\\":\\\"None\\\"}}\" \\\n    -- curl $HOSTNAME -v\n</code></pre>"},{"location":"local-kuar/","title":"Local playground with Kuar web app","text":"<p>Kuar is a web based (SPA) application that contains a couple of handy features that can verify the k8gb functionality. It can resolve DNS names, simulate probe failures, can simulate a CPU-intensive workload by generating RSA key pairs, and also contains a simple API for the queue (push &amp; pop strings).</p> <p>Make sure you have all the tools mentioned in this section installed.</p> <p>First, spin up two local k3s clusters:</p> <pre><code>make deploy-full-local-setup\n</code></pre> <p>Again, you can verify that everything is up and running by following the steps here.</p>"},{"location":"local-kuar/#deploy-the-kuar-app","title":"Deploy the Kuar app","text":"<pre><code>make deploy-kuar-app\n</code></pre> <p>This task will deploy Kuar into both clusters and exposes it. It also patches the installed nginx controller to serve the app even if the <code>Host</code> header is not provided in the request. This way you can access the application in your browser under http://localhost:80 for cluster 1 and http://localhost:81 for cluster 2.</p> <p>Make sure the app on http://localhost is responding, it may take a minute for the nginx ingress controller to restart with the correct parameters.</p> <p>The make target also modified the deployment of the Kuar application to use our core DNS servers. To verify that this was done, one can open the Kuar's file system browser tab and open <code>/etc/resolv.conv</code>. It should contain the same IP as cluster-IP assigned to <code>k8gb-coredns</code> service.</p> <p>Together with Kuar, we also prepared the failover gslb resource for k8gb. Where the first cluster (geotag = <code>eu</code>) is the primary one.</p>"},{"location":"local-kuar/#simulate-failure","title":"Simulate failure","text":"<p>Before we do that, we can visit Kuar on cluster 2 and verify that hostname <code>kuar.cloud.example.com</code> is correctly resolved IPs corresponding to nodes in the primary cluster - which is the cluster 1. This DNS resolution will work the same way on both clusters, but we will be simulating the failure on cluster 1 soon so the web API will not respond on cluster 1.</p> <p></p> <p>Now we can open a new tab with Kuar on cluster one and simulate the readiness probe failure for the next 10 checks - http://localhost/-/readiness.</p> <p></p> <p>Once we do that, the web API on cluster 1 will become unavailable. So we can switch to cluster 2 and run the web-based dig again. This time it should respond with IPs from cluster two. However, once the readiness probe will start succeeding again, it will switch back to cluster 1.</p> <p></p>"},{"location":"local/","title":"Local Setup","text":""},{"location":"local/#local-playground-for-testing-and-development","title":"Local playground for testing and development","text":"<ul> <li>Environment prerequisites</li> <li>Running project locally</li> <li>Verify installation</li> <li>Run integration tests</li> <li>Cleaning</li> <li>Sample demo</li> <li>Round Robin</li> <li>Failover</li> </ul> <p>NOTE</p> <p>This tutorial relies on some makefile targets, to be able to fully understand what's happening under the covers, check the Makefile source code. Or you may want to run a target first with <code>-n</code> switch that will print what it is going to do (example: <code>make -n test-round-robin</code>). For more user-centric targets in that makefile consult <code>make help</code>.</p>"},{"location":"local/#environment-prerequisites","title":"Environment prerequisites","text":"<ul> <li> <p>Install Go 1.22.3</p> </li> <li> <p>Install Git</p> </li> <li> <p>Install Docker</p> <p>Ensure you are able to push/pull from your docker registry</p> </li> </ul> <p>To run multiple clusters, reserve 8GB of memory <sup> above screenshot is provided for Docker for Mac, options for other Docker distributions may vary         </sup> <ul> <li> <p>install kubectl to operate k8s clusters</p> </li> <li> <p>install helm3 to deploy k8gb and related test workloads</p> </li> <li> <p>install k3d to run local k3s clusters (minimum v5.3.0 version is required)</p> </li> <li> <p>install golangci-lint for code quality checks</p> </li> </ul>"},{"location":"local/#running-project-locally","title":"Running project locally","text":"<p>To spin-up a local environment using two k3s clusters and deploy a test application to both clusters, execute the command below: <pre><code>make deploy-full-local-setup\n</code></pre></p>"},{"location":"local/#verify-installation","title":"Verify installation","text":"<p>If local setup runs well, check if clusters are correctly installed</p> <pre><code>kubectl cluster-info --context k3d-edgedns &amp;&amp; kubectl cluster-info --context k3d-test-gslb1 &amp;&amp; kubectl cluster-info --context k3d-test-gslb2\n</code></pre> <p>Cluster test-gslb1 is exposing external DNS on default port <code>:5053</code> while test-gslb2 on port <code>:5054</code>.</p> <p>Cluster edgedns runs BIND and acts as EdgeDNS holding Delegated Zone for out test setup and answers on port <code>:1053</code>.</p> <p><pre><code>dig @localhost -p 1053 roundrobin.cloud.example.com +short +tcp\n</code></pre> Should return two A records from both clusters (IP addresses and order may differ): <pre><code>172.20.0.2\n172.20.0.5\n172.20.0.4\n172.20.0.6\n</code></pre></p> <p>You can verify that correct IP addresses of all the nodes in both clusters were populated: <pre><code>for c in k3d-test-gslb{1,2}; do kubectl get no -ocustom-columns=\"NAME:.metadata.name,IP:status.addresses[0].address\" --context $c; done\n</code></pre></p> <p>Returns a result similar to: <pre><code>NAME                      IP\nk3d-test-gslb1-agent-0    172.20.0.2\nk3d-test-gslb1-server-0   172.20.0.4\nNAME                      IP\nk3d-test-gslb2-server-0   172.20.0.6\nk3d-test-gslb2-agent-0    172.20.0.5\n</code></pre></p> <p>Or you can ask specific CoreDNS instance for its local targets: <pre><code>dig -p 5053 +tcp @localhost localtargets-roundrobin.cloud.example.com &amp;&amp; \\\ndig -p 5054 +tcp @localhost localtargets-roundrobin.cloud.example.com\n</code></pre> As expected result you should see two A records divided between both clusters. <pre><code>...\n...\n;; ANSWER SECTION:\nlocaltargets-roundrobin.cloud.example.com. 30 IN A 172.20.0.4\nlocaltargets-roundrobin.cloud.example.com. 30 IN A 172.20.0.2\n...\n...\nlocaltargets-roundrobin.cloud.example.com. 30 IN A 172.20.0.5\nlocaltargets-roundrobin.cloud.example.com. 30 IN A 172.20.0.6\n</code></pre> Both clusters have podinfo installed on the top. Run following command and check if you get two json responses. <pre><code>curl localhost:80 -H \"Host:roundrobin.cloud.example.com\" &amp;&amp; curl localhost:81 -H \"Host:roundrobin.cloud.example.com\"\n</code></pre></p>"},{"location":"local/#run-integration-tests","title":"Run integration tests","text":"<p>There is wide range of scenarios which GSLB provides and all of them are covered within tests. To check whether everything is running properly execute terratest :</p> <pre><code>make terratest\n</code></pre>"},{"location":"local/#cleaning","title":"Cleaning","text":"<p>Clean up your local development clusters with <pre><code>make destroy-full-local-setup\n</code></pre></p>"},{"location":"local/#sample-demo","title":"Sample demo","text":""},{"location":"local/#round-robin","title":"Round Robin","text":"<p>Both clusters have podinfo installed on the top, where each cluster has been tagged to serve a different region. In this demo we will hit podinfo by <code>wget -qO - roundrobin.cloud.example.com</code> and depending on the region, podinfo will return us or eu. In the current round robin implementation IP addresses are randomly picked. See Gslb manifest with round robin strategy</p> <p>Try to run the following command several times and watch the <code>message</code> field. <pre><code>make test-round-robin\n</code></pre> As expected result you should see podinfo message changing</p> <p><pre><code>{\n  \"hostname\": \"frontend-podinfo-856bb46677-8p45m\",\n  ...\n  \"message\": \"us\",\n  ...\n}\n</code></pre> <pre><code>{\n  \"hostname\": \"frontend-podinfo-856bb46677-8p45m\",\n  ...\n  \"message\": \"eu\",\n  ...\n}\n</code></pre></p>"},{"location":"local/#failover","title":"Failover","text":"<p>Both clusters have podinfo installed on the top where each cluster has been tagged to serve a different region. In this demo we will hit podinfo by <code>wget -qO - failover.cloud.example.com</code> and depending on whether podinfo is running inside the cluster it returns only eu or us. See Gslb manifest with failover strategy</p> <p>Switch GLSB to failover mode: <pre><code>make init-failover\n</code></pre> Now both clusters are running in failover mode and podinfo is running on both of them. Run several times command below and watch <code>message</code> field. <pre><code>make test-failover\n</code></pre> You will see only eu podinfo is responsive: <pre><code>{\n  \"hostname\": \"frontend-podinfo-856bb46677-8p45m\",\n  ...\n  \"message\": \"eu\",\n  ...\n}\n</code></pre> Stop podinfo on current (eu) cluster: <pre><code>make stop-test-app\n</code></pre> Several times hit application again <pre><code>make test-failover\n</code></pre> As expected result you should see only podinfo from second cluster (us) is responding: <pre><code>{\n  \"hostname\": \"frontend-podinfo-856bb46677-v5nll\",\n  ...\n  \"message\": \"us\",\n  ...\n}\n</code></pre> It might happen that podinfo will be unavailable for a while due to DNS sync interval and default k8gb DNS TTL of 30 seconds <pre><code>wget: server returned error: HTTP/1.1 503 Service Temporarily Unavailable\n</code></pre> Start podinfo again on current (eu) cluster: <pre><code>make start-test-app\n</code></pre> and hit several times hit podinfo: <pre><code>make test-failover\n</code></pre> After DNS sync interval is over eu will be back <pre><code>{\n  \"hostname\": \"frontend-podinfo-6945c9ddd7-xksrc\",\n  ...\n  \"message\": \"eu\",\n  ...\n}\n</code></pre> Optionally you can switch GLSB back to round-robin mode <pre><code>make init-round-robin\n</code></pre></p>"},{"location":"metrics/","title":"Metrics","text":"<p>K8GB generates Prometheus-compatible metrics. Metrics endpoints are exposed via <code>-metrics</code> service in operator namespace and can be scraped by 3rd party tools:</p> <pre><code>spec:\n...\n  ports:\n  - name: http-metrics\n    port: 8383\n    protocol: TCP\n    targetPort: 8383\n  - name: cr-metrics\n    port: 8686\n    protocol: TCP\n    targetPort: 8686\n</code></pre> <p>Metrics can be also automatically discovered and monitored by Prometheus Operator via automatically generated ServiceMonitor CRDs , in case if Prometheus Operator  is deployed into the cluster.</p>"},{"location":"metrics/#general-metrics","title":"General metrics","text":"<p>controller-runtime standard metrics, extended with K8GB operator-specific metrics listed below:</p>"},{"location":"metrics/#healthy_records","title":"<code>healthy_records</code>","text":"<p>Number of healthy records observed by K8GB.</p> <p>Example:</p> <pre><code># HELP k8gb_gslb_healthy_records Number of healthy records observed by K8GB.\n# TYPE k8gb_gslb_healthy_records gauge\nk8gb_gslb_healthy_records{name=\"test-gslb\",namespace=\"test-gslb\"} 6\n</code></pre>"},{"location":"metrics/#ingress_hosts_per_status","title":"<code>ingress_hosts_per_status</code>","text":"<p>Number of ingress hosts per status (NotFound, Healthy, Unhealthy), observed by K8GB.</p> <p>Example:</p> <pre><code># HELP k8gb_gslb_ingress_hosts_per_status Number of managed hosts observed by K8GB.\n# TYPE k8gb_gslb_ingress_hosts_per_status gauge\nk8gb_gslb_ingress_hosts_per_status{name=\"test-gslb\",namespace=\"test-gslb\",status=\"Healthy\"} 1\nk8gb_gslb_ingress_hosts_per_status{name=\"test-gslb\",namespace=\"test-gslb\",status=\"NotFound\"} 1\nk8gb_gslb_ingress_hosts_per_status{name=\"test-gslb\",namespace=\"test-gslb\",status=\"Unhealthy\"} 2\n</code></pre> <p>Served on <code>0.0.0.0:8383/metrics</code> endpoint</p>"},{"location":"metrics/#custom-resource-specific-metrics","title":"Custom resource specific metrics","text":"<p>Info metrics, automatically exposed by operator based on the number of the current instances of an operator's custom resources in the cluster.</p> <p>Example:</p> <pre><code># HELP gslb_info Information about the Gslb custom resource.\n# TYPE gslb_info gauge\ngslb_info{namespace=\"test-gslb\",gslb=\"test-gslb\"} 1\n</code></pre> <p>Served on <code>0.0.0.0:8686/metrics</code> endpoint</p>"},{"location":"metrics/#metrics_1","title":"Metrics","text":"<p>The k8gb exposes several metrics to help you monitor the health and behavior.</p> Metric Type Description Labels <code>k8gb_gslb_errors_total</code> Counter Number of errors <code>namespace</code>, <code>name</code> <code>k8gb_gslb_healthy_records</code> Gauge Number of healthy records observed by k8gb. <code>namespace</code>, <code>name</code> <code>k8gb_gslb_reconciliation_loops_total</code> Counter Number of successful reconciliation loops. <code>namespace</code>, <code>name</code> <code>k8gb_gslb_service_status_num</code> Gauge Number of managed hosts observed by k8gb. <code>namespace</code>, <code>name</code>, <code>status</code> <code>k8gb_gslb_status_count_for_failover</code> Gauge Gslb status count for Failover strategy. <code>namespace</code>, <code>name</code>, <code>status</code> <code>k8gb_gslb_status_count_for_geoip</code> Gauge Gslb status count for GeoIP strategy. <code>namespace</code>, <code>name</code>, <code>status</code> <code>k8gb_gslb_status_count_for_roundrobin</code> Gauge Gslb status count for RoundRobin strategy. <code>namespace</code>, <code>name</code>, <code>status</code> <code>k8gb_infoblox_heartbeat_errors_total</code> Counter Number of k8gb Infoblox TXT record errors. <code>namespace</code>, <code>name</code> <code>k8gb_infoblox_heartbeats_total</code> Counter Number of k8gb Infoblox heartbeat TXT record updates. <code>namespace</code>, <code>name</code> <code>k8gb_infoblox_request_duration</code> Histogram Duration of the HTTP request to Infoblox API in seconds. <code>request</code>, <code>success</code> <code>k8gb_infoblox_zone_update_errors_total</code> Counter Number of k8gb Infoblox zone update errors. <code>namespace</code>, <code>name</code> <code>k8gb_infoblox_zone_updates_total</code> Counter Number of k8gb Infoblox zone updates. <code>namespace</code>, <code>name</code> <code>k8gb_endpoint_status_num</code> Gauge Number of targets in DNS endpoint. <code>namespace</code>, <code>name</code>, <code>dns_name</code> <code>k8gb_runtime_info</code> Gauge K8gb runtime info. <code>namespace</code>, <code>k8gb_version</code>, <code>go_version</code>, <code>arch</code>, <code>os</code>, <code>git_sha</code>"},{"location":"metrics/#opentracing","title":"OpenTracing","text":"<p>Optionally k8gb operator can expose traces in OpenTelemetry format to any available OTEL compliant tracing solution. Consult the following page for more details.</p>"},{"location":"multizone/","title":"Multizone Support","text":"<p>Starting with v0.15.0, k8gb supports managing more than one DNS zone in a single deployment.</p>"},{"location":"multizone/#why-multizone","title":"Why multizone?","text":"<p>Previously, k8gb could only manage a single DNS zone per deployment. If you needed to handle multiple zones (for example, for several domains or environments), you had to deploy several instances of k8gb, each in a separate namespace, and carefully coordinate their configuration. This approach was difficult to manage and prone to errors, especially with Helm charts and CRDs.</p> <p>To simplify things, k8gb now supports defining multiple zones directly in your configuration.</p>"},{"location":"multizone/#how-to-configure-multiple-zones","title":"How to configure multiple zones","text":"<p>The new configuration uses the dnsZones field (in your values.yaml or Helm values), where you can define a list of zones to be managed by a single k8gb deployment.</p> <pre><code>k8gb:\n  dnsZones:\n    - parentZone: \"example.com\"             # The parent zone which delegates to the GSLB zone (previously edgeDNSZone)\n      loadBalancedZone: \"cloud.example.com\" # The zone actually managed by GSLB (previously dnsZone)\n      dnsZoneNegTTL: 30                     # Negative TTL for SOA records\n    - parentZone: \"example.org\"\n      loadBalancedZone: \"cloud.example.org\"\n      dnsZoneNegTTL: 30\n    - parentZone: \"example.org\"\n      loadBalancedZone: \"onprem.example.org\"\n      dnsZoneNegTTL: 30\n</code></pre> <p>Note:The field names have also changed for clarity:  - <code>edgeDNSZone</code> is now called <code>parentZone</code>  - <code>dnsZone</code> is now called <code>loadBalancedZone</code></p> <p>For backward compatibility, the dnsZone and edgeDNSZone fields are allowed; otherwise, the dnsZones array is used. For valid values, use either dnsZone and edgeDNSZone or dnsZones.We recommend switching to the new syntax for all new deployments.</p>"},{"location":"provider_rfc2136/","title":"Enabling RFC2136 for ExternalDNS","text":"<p>In order to enable the provider RFC2136 on ExternalDNS, the following <code>rfc2136.*</code> parameters should be changed in the values.yaml of the K8GB helm chart:</p> <ul> <li>One authentication method should be enabled on the values:</li> <li>Insecure<ul> <li>This method doesn't use any authentication and anonymous updates to the DNS records can be executed</li> </ul> </li> <li>TSIG<ul> <li>This method uses TSIG authentication that relies on a token provided for the DNS records update.</li> </ul> </li> <li> <p>GSS-TSIG</p> <ul> <li>This method uses GSS-TSIG authentication, which is a variation of the TSIG method, but uses Kerberos for the generation of tokens for authentication and authorization</li> <li>Method used by Active Directory Windows DNS</li> </ul> </li> <li> <p>GSS-TSIG</p> </li> <li>kerberos-username<ul> <li>this key should have the value of a Active Directory user account that has permissions for DNS updates</li> </ul> </li> <li>kerberos-password<ul> <li>password of the user account that will be used. Be aware that this isn't encrypted and so far ExternalDNS doesn't support adding a Secret reference for this value, so it will be stored in clear text</li> </ul> </li> <li>kerberos-realm<ul> <li>domain that will be used for authentication of the user</li> </ul> </li> </ul>"},{"location":"provider_rfc2136/#sample-for-gss-tsig-authentication","title":"Sample for GSS-TSIG authentication","text":"<pre><code>rfc2136:\n  enabled: true\n  rfc2136Opts:\n    - host: yourAcDc.k8gb.local\n    - port: 53\n  rfc2136auth:\n    insecure:\n      enabled: false\n    tsig:\n      enabled: false\n      tsigCreds:\n        - tsig-secret-alg: hmac-sha256\n        - tsig-keyname: externaldns-key\n    gssTsig:\n      enabled: true\n      gssTsigCreds:\n        - kerberos-username: someServiceAccount\n        - kerberos-password: insecurePlainTextPassword\n        - kerberos-realm: yourKerberosRealm.domain\n</code></pre>"},{"location":"proxy_externaldns/","title":"External DNS behind a proxy","text":"<p>External DNS needs to communicate with a DNS server outside of the kubernetes cluster to update records. If a proxy is used for egress from the Kubernetes cluster the following should be configured: <pre><code>externaldns:\n  extraEnv:\n  - name: HTTPS_PROXY\n    value: http://proxy.example.com:8080\n  extraVolumes:\n  - name: ca-bundle\n    secret:\n      secretName: ca-proxy\n  extraVolumeMounts:\n  - name: ca-bundle\n    mountPath: /etc/ssl/certs\n    readOnly: true\n</code></pre></p> <p>The <code>HTTPS_PROXY</code> environment variable should contain the address of the proxy. The volume mount should contain the proxy CA certificate so that the container can trust the proxy.</p>"},{"location":"rancher/","title":"Integration with Rancher Fleet","text":"<p>The K8gb has been modified to be easily deployed using Rancher Fleet. All you need to supply is a  fleet.yaml file  and possibly expose the labels on your cluster.</p>"},{"location":"rancher/#deploy-k8gb-to-target-clusters","title":"Deploy k8gb to Target clusters","text":"<p>The following shows the rancher application that will be installed on the target cluster.  The values <code>k8gb-dnsZone</code>,  <code>k8gb-clusterGeoTag</code>, <code>k8gb-extGslbClustersGeoTags</code> will be taken from the labels that are set on the cluster.</p> <pre><code># fleet.yaml\ndefaultNamespace: k8gb\nkustomize:\n  dir: overlays/kustomization\nlabels:\n  bundle: k8gb\nhelm:\n  repo: https://www.k8gb.io\n  chart: k8gb\n  version: v0.11.4\n  releaseName: k8gb\n  values:\n    k8gb:\n      dnsZones:\n        - parentZone: cloud.example.com\n          loadBalancedZone: global.fleet.clusterLabels.k8gb-dnsZone\n      parentZoneDNSServers:\n        - \"1.2.3.4\"\n        - \"5.6.7.8\"\n      clusterGeoTag: global.fleet.clusterLabels.k8gb-clusterGeoTag\n      extGslbClustersGeoTags: global.fleet.clusterLabels.k8gb-extGslbClustersGeoTags\n      log:\n        format: simple\n</code></pre>"},{"location":"resource_ref/","title":"GSLB ResourceRef Support","text":"<p>Starting from v0.15.0, k8gb introduces a much simpler way to link a GSLB resource to an Ingress object in Kubernetes. You no longer need to duplicate the Ingress configuration in your GSLB definition\u2014instead, you can simply reference an existing Ingress.  This makes your Ingress the single source of truth for application routing.</p>"},{"location":"resource_ref/#1-declaration-by-name","title":"1. Declaration by Name","text":"<p>The simplest way is to directly specify the name of the Ingress you want to reference in your GSLB. The namespace will be automatically taken from the GSLB\u2019s namespace.</p> <pre><code>apiVersion: k8gb.absa.oss/v1beta1\nkind: Gslb\nmetadata:\n  name: playground-failover\nspec:\n  resourceRef:\n    apiVersion: networking.k8s.io/v1\n    kind: Ingress\n    name: playground-failover-ingress\n</code></pre>"},{"location":"resource_ref/#2-declaration-by-label","title":"2. Declaration by Label","text":"<p>Alternatively, you can reference the Ingress by label. This approach is useful when you need more flexibility\u2014for example,  in CI/CD pipelines. It is required that only one Ingress in the namespace matches the label; otherwise, k8gb will return  an error.</p> <pre><code>apiVersion: k8gb.absa.oss/v1beta1\nkind: Gslb\nmetadata:\n  name: playground-failover\nspec:\n  resourceRef:\n    apiVersion: networking.k8s.io/v1\n    kind: Ingress\n    matchLabels:\n      app: playground-failover\n</code></pre>"},{"location":"resource_ref/#3-embedded-declaration-legacy","title":"3. Embedded Declaration (Legacy)","text":"<p>For backward compatibility, you can still use the original way where the Ingress configuration is embedded directly inside the GSLB resource. This method will continue to work, but we recommend switching to reference-based configuration for simpler management and to avoid configuration drift.</p> <pre><code>apiVersion: k8gb.absa.oss/v1beta1\nkind: Gslb\nmetadata:\n  name: failover-playground-embedded\nspec:\n  ingress:\n    ingressClassName: nginx\n    rules:\n      - host: failover-playground-embedded.cloud.example.com\n        http:\n          paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: frontend-podinfo\n                port:\n                  name: http\n  strategy:\n    type: failover\n    dnsTtlSeconds: 5\n    primaryGeoTag: \"eu\"\n</code></pre> <p>Note: If the Ingress is created automatically by a GSLB resource, in addition to an ownerReference, it will also be marked with the label: <code>app.k8gb.io/managed-by: gslb</code>. This makes it easy to identify Ingresses managed by k8gb.</p>"},{"location":"service_upgrade/","title":"Service Upgrade","text":""},{"location":"service_upgrade/#changing-coredns-service-type","title":"Changing CoreDNS service type","text":"<p>Since Kubernetes API doesn't allow to change service type. Helm chart provides default label <code>k8gb-migrated-svc</code>, when set during helm upgrade, old service deleted by helm upgrade hook in automatic manner. Here is the label pattern for service deletion <code>app.kubernetes.io/name=coredns,k8gb-migrated-svc!=true</code></p>"},{"location":"strategy/","title":"k8gb strategy options","text":""},{"location":"strategy/#roundrobin","title":"RoundRobin","text":"<p>Returns both cluster endpoints in round-robin manner.</p>"},{"location":"strategy/#weight-roundrobin","title":"Weight RoundRobin","text":"<p>While roundRobin is fair for all regions, with WeightRoundRobin we can set explicitly how the regions should be loaded with traffic. For example, we can set one region to handle 80% of the traffic, another 20% and a third 0%, so that the last region is practically disabled.</p> <p>Note: Please read the WRR caveatsfor important limitations and recommendations.</p>"},{"location":"strategy/#failover","title":"Failover","text":"<p>Pinned to a specified primary cluster until workload on that cluster has no available Pods, upon which the next available cluster's Ingress node IPs will be resolved. When Pods are again available on the primary cluster, the primary cluster will once again be the only eligible cluster for which cluster Ingress node IPs will be resolved</p>"},{"location":"strategy/#geoip","title":"GeoIP","text":"<p>Similar to <code>failover</code> mode, but returns \"closest\" cluster to the client initiating request. This requires a specially crafted GeoIP database (see this for example) and DNS resolver to support EDNS0 extension (CLIENT-SUBNET in particular). If the client subnet is not in GeoIP Database, all available endpoints are returned</p>"},{"location":"traces/","title":"Traces","text":"<p>K8GB can create and send spans/traces to either Jaeger which is supported also on the Helm chart level or to any other OTEL compliant solution. We don't recommend sending the tracing data directly to the tracer, because of the possible vendor lock-in. OTEL collector can be deployed as a side-car container for k8gb pod and forward all the traces to a configurable sink or event multiple sinks for redundancy.</p>"},{"location":"traces/#architecture","title":"Architecture","text":"<p>We are not opinionated about the OpenTracing vendors. In the following diagram <code>X</code> can be Jaeger, Zipkin, LigthStep, Grafana's Tempo, Instana, etc. It can also be another OTEL collector to form more sophisticated pipeline setup.</p> <p>Sidecar use-case:</p> <pre><code>+--------------+               +------------+             +----------+\n|     k8gb     |               |     X      |    http     |   User   |\n| ------------ |     otlp      |            +------------&gt;|          |\n| OTEL sidecar +--------------&gt;|            |             |          |\n+--------------+               +------------+             +----------+\n</code></pre>"},{"location":"traces/#deployment","title":"Deployment","text":"<p>By default the tracing is disabled and no sidecar container is being created during the k8gb deployment. To enable the tracing, one has to set the <code>tracing.enabled=true</code> in Helm Chart. This will create the sidecar container for k8gb deployment, tweaks couple of env vars there. It will create the configmap for OTEL sidecar. This configuration of OTEL collector can be overridden by <code>tracing.otelConfig</code>.</p> <p>If you need something quickly up and running, make sure that <code>tracing.deployJaeger</code> is also set to <code>true</code>. In this scenario you will end up also with Jaeger deployed and service for it. To be able to access it one can continue with:</p> <pre><code>kubectl port-forward svc/jaeger-collector 16686\nopen http://localhost:16686\n</code></pre> <p>Also both sidecar container image and jaeger deployment's container image can be tweaked by <code>tracing.{sidecarImage,jaegerImage}.{repository,tag,pullPolicy}</code>.</p>"},{"location":"traces/#custom-architecture","title":"Custom Architecture","text":"<p>In case you have already a OTEL collector present in the Kubernetes cluster and you do not want to introduce a new one, you can deploy also the following topology:</p> <pre><code>+----------+               +----------------+             +--------+             +---------+\n|   k8gb   |               | OTEL collector |    otlp     |   X    |    http     |  User   |\n|          |     otlp      |                +------------&gt;|        +------------&gt;|         |\n|          +--------------&gt;|                |             |        |             |         |\n+----------+               +----------------+             +--------+             +---------+\n</code></pre> <p>However, we don't support this use-case on the Helm chart level so you are on your own with the setup. Nonetheless, it should be relatively straightforward. All you have to do is set the following env vars for k8gb deployment:  - <code>TRACING_ENABLED</code> (set it to <code>true</code>)  - <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> (<code>host:port</code> of OTEL collector from the ASCII diagram)  - <code>TRACING_SAMPLING_RATIO</code> (optional, float representing the % ratio of how many traces are being collected)</p>"},{"location":"traces/#distributed-tracing-context-propagation","title":"Distributed Tracing &amp; Context Propagation","text":"<p>K8gb is a k8s controller/operator so in nature event-based system, where the invocation of the request is done using creating custom resource (<code>gslb</code>) or <code>Ingress</code> with certain annotation. In more traditional micro-service world, the context propagation between traced systems is done using HTTP headers. Provided that the comm client is also instrumented with OpenTracing bits one doesn't have to call the <code>Extract</code> and  <code>Inject</code> on his own. However, for the CRD space nothing has been introduced so far and having the tracing  key-value metadata stored in each custom resource would be overkill here. Not speaking about increasing the  overall complexity of such a system.</p> <p>If k8gb had a REST api, it would be a very low-hanging fruit on the other hand.</p> <p>As for the propagation of context down for the calls that k8gb does, this may make sense for direct HTTP  calls to external systems such as Infoblox or Route53. Then provided that those systems also support  OpenTracing and they have the context propagation done right on their part, one could see the full insight  into the requests and examine what takes most of the time or where the issue happened. As for communication  with <code>ExternalDNS</code>, it's again on the \"CRD level\" -&gt; hard to achieve + the ExternalDNS operator is not traced.</p>"},{"location":"tutorials/","title":"Installation and Configuration Tutorials","text":"<p>This section provides comprehensive tutorials for deploying and configuring K8GB with various DNS providers and environments.</p>"},{"location":"tutorials/#dns-provider-integrations","title":"DNS Provider Integrations","text":"<ul> <li>General deployment with Infoblox integration</li> <li>AWS based deployment with Route53 integration</li> <li>AWS based deployment with NS1 integration</li> <li>Using Azure Public DNS provider</li> <li>Azure based deployment with Windows DNS integration</li> <li>General deployment with Cloudflare integration</li> <li>Seamless DDNS Integration with Bind9 and other RFC2136-Compatible DNS Environments</li> </ul>"},{"location":"tutorials/#development-and-testing","title":"Development and Testing","text":"<ul> <li>Local playground for testing and development</li> <li>Local playground with Kuar web app</li> </ul>"},{"location":"tutorials/#monitoring-and-observability","title":"Monitoring and Observability","text":"<ul> <li>Metrics</li> <li>Traces</li> </ul>"},{"location":"tutorials/#configuration","title":"Configuration","text":"<ul> <li>Ingress annotations</li> <li>Resource References</li> <li>Address Discovery</li> <li>Dynamic Geotags</li> <li>Multi-zone Setup</li> <li>Exposing DNS</li> </ul>"},{"location":"tutorials/#platform-integrations","title":"Platform Integrations","text":"<ul> <li>Integration with Admiralty</li> <li>Integration with Liqo</li> <li>Integration with Rancher Fleet</li> </ul>"},{"location":"tutorials/#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Service Upgrade</li> <li>WRR Caveats</li> <li>External DNS Proxy</li> </ul>"},{"location":"wrr_caveats/","title":"Weighted Round Robin caveats","text":"<p>k8gb successfully implements weighted round robin (WRR) at the level of authoritative CoreDNS servers. The correct weighted records are published by each cluster\u2019s CoreDNS instance as the authoritative DNS server.</p> <p>However, in most production setups, DNS queries are first answered by a parent DNS (sometimes referred to as a delegation DNS), which acts as an upper-level authoritative server delegating queries to cluster CoreDNS servers. The actual order and frequency in which DNS answers are returned to clients depends on this delegation DNS (e.g., BIND, Infoblox, Unbound, etc.), which may also act as a caching or forwarding layer.</p> <p>Many delegation (parent) DNS servers will randomize or reorder A/NS records, or apply their own internal load-balancing or optimizations (such as sorting by RTT). As a result, the expected traffic split according to weights may not be reflected on the client side, even though k8gb and CoreDNS publish the correct weighted records. This is out of the scope of K8gb.</p> <p>If you need strict control over record ordering or weight distribution, you have these options:</p> <ul> <li> <p>Review and configure your parent DNS server to ensure it does not shuffle or reorder answers, or that it supports weighted policies appropriately (see your DNS server documentation for options).</p> </li> <li> <p>Bypass parent DNS and integrate directly with your custom application logic to implement advanced load-balancing based on your needs.</p> </li> </ul> <p>For more information and real-world examples, see the following k8gb GitHub issues:</p> <ul> <li>Weighted round robin DNS limitations #1950</li> <li>Further discussion on WRR behavior #1943</li> </ul>"},{"location":"examples/crossplane/globalapp/","title":"Resilient Multiregion Global Control Planes With Crossplane and K8gb","text":"<p>This example demonstrates how to build resilient, scalable multicluster environments using Crossplane's declarative infrastructure provisioning integrated with k8gb for DNS-based failover.</p>"},{"location":"examples/crossplane/globalapp/#overview","title":"Overview","text":"<p>This reference architecture showcases a Crossplane-based Global Control Plane with Active/Passive setup:</p> <ul> <li>Active/Passive Control Planes: Multiple regions with one active control plane and passive standby</li> <li>DNS-based Failover: k8gb provides automatic DNS failover when active region fails</li> <li>GSLB Health Monitoring: Crossplane observes GSLB resources to track control plane health</li> <li>Automated Failover: Passive control plane transitions to Active during failures</li> </ul>"},{"location":"examples/crossplane/globalapp/#key-components","title":"Key Components","text":"<ul> <li>Composition Pipeline: Uses KCL function to observe GSLB resources and report health status</li> <li>GSLB Observation: Monitors k8gb.absa.oss/v1beta1 Gslb resources in observe-only mode</li> <li>Health Status Integration: Extracts serviceHealth from GSLB and updates XR status for failover decisions</li> <li>Multiregion Coordination: Enables Crossplane to make intelligent decisions based on regional health</li> </ul>"},{"location":"examples/crossplane/globalapp/#files","title":"Files","text":"<ul> <li><code>composition.yaml</code>: Crossplane Composition with KCL function for GSLB health monitoring</li> <li><code>definition.yaml</code>: CompositeResourceDefinition with GSLB status schema for failover logic</li> <li><code>xr.yaml</code>: Example XR instance representing a control plane endpoint</li> <li><code>xr-auto.yaml</code>: Auto-mode XR example with intelligent GSLB-based failover</li> <li><code>xr-passive.yaml</code>: Passive-mode XR example for standby regions</li> <li><code>functions.yaml</code>: KCL function package for health monitoring logic</li> <li><code>provider-*.yaml</code>: Provider configurations for multicluster access</li> </ul>"},{"location":"examples/crossplane/globalapp/#usage","title":"Usage","text":""},{"location":"examples/crossplane/globalapp/#local-testing","title":"Local Testing","text":"<pre><code># Render the composition locally\nmake run\n\n# Or using crossplane render directly\ncrossplane render xr.yaml composition.yaml functions.yaml -r\n</code></pre>"},{"location":"examples/crossplane/globalapp/#local-installation","title":"Local Installation","text":"<pre><code># In the main k8gb repo\nmake deploy-full-local-setup\n</code></pre> <pre><code># Check current context. We expect second k8gb cluster to be active after the\n# installation\nk config current-context\nk3d-test-gslb2\n</code></pre> <pre><code># Install Crossplane\nhelm install crossplane \\\n--namespace crossplane-system \\\n--create-namespace crossplane-stable/crossplane\n</code></pre> <pre><code># Install Crossplane Providers\nk apply -f crossplane-providers\n</code></pre> <pre><code># Wait for providers readiness\nk get providers\nNAME                            INSTALLED   HEALTHY   PACKAGE                                                 AGE\nprovider-azure-cache            True        True      xpkg.upbound.io/upbound/provider-azure-cache:v1         35s\nprovider-helm                   True        True      xpkg.upbound.io/upbound/provider-helm:v0                35s\nprovider-kubernetes             True        True      xpkg.upbound.io/upbound/provider-kubernetes:v0          35s\nupbound-provider-family-azure   True        True      xpkg.upbound.io/upbound/provider-family-azure:v1.13.0   30s\n</code></pre> <pre><code># Install ProviderConfigs\nk apply -f crossplane-providers/providerconfigs\nproviderconfig.azure.upbound.io/default created\nproviderconfig.helm.crossplane.io/default created\nproviderconfig.kubernetes.crossplane.io/default created\n</code></pre> <pre><code># Setup Azure Credentials Secret\nkubectl create secret generic azure-account-creds -n crossplane-system --from-literal=credentials=\"$(cat credentials.json)\" --dry-run=client -o yaml | kubectl apply -f -\n# If you don't have Azure credentials json file, check Quickstart documentation\nat https://marketplace.upbound.io/providers/upbound/provider-family-azure/latest\n</code></pre> <pre><code># Apply demo GlobalApp material\nk apply -f definition.yaml,composition.yaml,functions.yaml\ncompositeresourcedefinition.apiextensions.crossplane.io/globalapps.example.crossplane.io created\nWarning: CustomResourceDefinition.apiextensions.k8s.io \"GlobalApp.example.crossplane.io\" not found\ncomposition.apiextensions.crossplane.io/global-app created\nfunction.pkg.crossplane.io/crossplane-contrib-function-kcl created\nfunction.pkg.crossplane.io/crossplane-contrib-function-auto-ready created\n</code></pre> <pre><code># Switch context to first k8gb cluster\nk config use-context k3d-test-gslb1\nSwitched to context \"k3d-test-gslb1\".\n# Repeat all installation steps in identical manner on the first cluster!\n</code></pre>"},{"location":"examples/crossplane/globalapp/#deploy-examples","title":"Deploy Examples","text":"<pre><code># Deploy the basic XR\nkubectl apply -f xr.yaml\n\n# Deploy auto-mode XR with intelligent failover\nkubectl apply -f xr-auto.yaml\n\n# Deploy passive-mode XR for standby regions\nkubectl apply -f xr-passive.yaml\n</code></pre>"},{"location":"examples/crossplane/globalapp/#architecture-details","title":"Architecture Details","text":"<p>The KCL function implements the core failover monitoring logic:</p> <ol> <li>Observe GSLB: Creates Kubernetes Object to monitor GSLB resource health</li> <li>Health Assessment: Extracts serviceHealth status from k8gb</li> <li>Status Propagation: Updates XR status to reflect regional control plane health (Healthy/UNHEALTHY)</li> <li>Failover Enablement: Provides health data for automated Active/Passive transitions</li> </ol> <p>The composition uses <code>managementPolicies: [\"Observe\"]</code> for read-only health monitoring without modifying the underlying GSLB resources, enabling safe observation across multiple regions.</p>"},{"location":"examples/crossplane/globalapp/#important-notes","title":"Important Notes","text":"<ul> <li>The domain is fully parameterized via <code>spec.hostname</code> for production flexibility</li> <li>Health monitoring is performed in observe-only mode to avoid conflicts across regions</li> <li>GSLB status updates drive automated failover decisions between Active/Passive control planes</li> <li>Auto-apply policy mode enables intelligent management policy switching based on GSLB health</li> </ul>"}]}